
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://tesla-cat.github.io/">
      
      
      
      
      <link rel="icon" href="assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>Ricky's notes</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.06209087.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#summary-of-deep-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Ricky&#39;s notes" class="md-header__button md-logo" aria-label="Ricky's notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ricky's notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Home
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Ricky&#39;s notes" class="md-nav__button md-logo" aria-label="Ricky's notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Ricky's notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Home
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-1-intro-and-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 1: Intro and overview
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 1: Intro and overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beyond-learning-from-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Beyond learning from reward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-2-supervised-learning-of-behaviors" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 2: Supervised Learning of behaviors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-3-pytorch-tutorial" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 3: PyTorch tutorial
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-4-intro-to-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 4: Intro to RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 4: Intro to RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-rl-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Types of RL algorithms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-5-policy-gradients" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 5: Policy Gradients
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 5: Policy Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#improvement-1-reward-to-go" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 1: reward to go
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-2-subtracting-a-reward-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 2: subtracting a reward baseline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-3-from-on-policy-to-off-policy-pg" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 3: from on-policy to off-policy PG
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-4-natural-pg-rescaling-the-vanilla-pg" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 4: Natural PG: rescaling the Vanilla PG
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-6-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 6: Actor-Critic
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 6: Actor-Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#improvement-5-actor-critic-still-trying-to-reduce-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 5: Actor-Critic: still trying to reduce variance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-6-from-on-policy-to-off-policy-ac" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 6: from on-policy to off-policy AC
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-1-intro-and-overview" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 1: Intro and overview
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 1: Intro and overview">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#beyond-learning-from-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Beyond learning from reward
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-2-supervised-learning-of-behaviors" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 2: Supervised Learning of behaviors
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-3-pytorch-tutorial" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 3: PyTorch tutorial
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-4-intro-to-rl" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 4: Intro to RL
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 4: Intro to RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#types-of-rl-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Types of RL algorithms
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-5-policy-gradients" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 5: Policy Gradients
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 5: Policy Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#improvement-1-reward-to-go" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 1: reward to go
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-2-subtracting-a-reward-baseline" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 2: subtracting a reward baseline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-3-from-on-policy-to-off-policy-pg" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 3: from on-policy to off-policy PG
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-4-natural-pg-rescaling-the-vanilla-pg" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 4: Natural PG: rescaling the Vanilla PG
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#lecture-6-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      Lecture 6: Actor-Critic
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Lecture 6: Actor-Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#improvement-5-actor-critic-still-trying-to-reduce-variance" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 5: Actor-Critic: still trying to reduce variance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improvement-6-from-on-policy-to-off-policy-ac" class="md-nav__link">
    <span class="md-ellipsis">
      Improvement 6: from on-policy to off-policy AC
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<!-- cspell:ignore Markovian, nonumber -->

<h1 id="summary-of-deep-reinforcement-learning">Summary of Deep Reinforcement Learning</h1>
<ul>
<li><a href="https://rail.eecs.berkeley.edu/deeprlcourse">CS285 at UC Berkeley</a><ul>
<li><a href="https://www.youtube.com/watch?v=SupFHGbytvA&amp;list=PL_iWQOsE6TfVYGEGiAOMaOzzv41Jfm_Ps">videos</a></li>
</ul>
</li>
</ul>
<h2 id="lecture-1-intro-and-overview">Lecture 1: Intro and overview</h2>
<h3 id="beyond-learning-from-reward">Beyond learning from reward</h3>
<ul>
<li>basic RL: maximize rewards</li>
<li>other methods:<ul>
<li><code>Inverse RL</code>: learn reward function from example</li>
<li><code>Transfer Learning</code>: transfer knowledge between domains</li>
<li><code>Meta learning</code>: learning to learn</li>
<li><code>Predicting</code>: use predictions to act</li>
</ul>
</li>
</ul>
<h2 id="lecture-2-supervised-learning-of-behaviors">Lecture 2: Supervised Learning of behaviors</h2>
<ul>
<li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li>
<li>solution: generate examples of "mistakes" and their "corrections" (<strong>teach "what didn't work and how to fix", not "what worked"</strong>)</li>
</ul>
<h2 id="lecture-3-pytorch-tutorial">Lecture 3: PyTorch tutorial</h2>
<h2 id="lecture-4-intro-to-rl">Lecture 4: Intro to RL</h2>
<ul>
<li>expanding the total reward over trajectory $ \tau = (s_1, a_1, s_2, a_2 \ldots) $:</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
    J &amp;= E_\tau \underbrace{ \sum_t r(s_t, a_t) }_{ r(\tau) }  \\
    &amp;= E_{s_1} \underbrace{
        E_{a_1} \underbrace{
            r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \ldots
        }_{ Q(s_1, a_1) }
    }_{V(s_1)} 
\end{align*}
\]</div>
<h3 id="types-of-rl-algorithms">Types of RL algorithms</h3>
<p><img alt="" src="imgs/rl_types.png" /></p>
<ul>
<li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li>
<li><code>on policy</code>: once the policy is changed, need to generate new samples</li>
</ul>
<p><img alt="" src="imgs/rl_efficiency.png" /></p>
<ul>
<li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li>
<li><code>Model based</code>: minimizes error of fit, but better model != better policy</li>
<li><code>Policy Gradient</code>: gradient descent on true objective</li>
</ul>
<h2 id="lecture-5-policy-gradients">Lecture 5: Policy Gradients</h2>
<div class="arithmatex">\[
\begin{align*}
J(\theta) &amp;= E_\tau r(\tau) = \int p_\theta(\tau) \; r(\tau) \; d\tau  \\
\nabla_\theta J &amp;= \int \nabla p \; r(\tau) \; d\tau = 
\int p \nabla \log p \; r(\tau) \; d\tau  = 
E_\tau \underbrace{ \nabla \log p }_{ \sum_t \nabla \log \pi_\theta(a_t | s_t) } \; r(\tau) \\
\text{because} \; p &amp;= p(s_1) \prod_t \pi_\theta(a_t | s_t) \; p(s_{t+1} | s_t, a_t)
\end{align*}
\]</div>
<p>notation $ \nabla \log \pi(\tau) := \sum_t \nabla \log \pi_\theta(a_t | s_t) $ </p>
<ul>
<li>approximate with sample mean:</li>
</ul>
<div class="arithmatex">\[
\nabla J \approx {1\over N} \sum_n \left( \sum_{t=1}^T \nabla \log \pi(a_{n, t} | s_{n, t}) \right)  \left( \sum_{t=1}^T r(s_{n, t}, a_{n, t}) \right)
\]</div>
<h3 id="improvement-1-reward-to-go"><code>Improvement 1</code>: reward to go</h3>
<ul>
<li><strong>causality</strong>: actions only affect the future, remove past rewards</li>
</ul>
<div class="arithmatex">\[
\nabla J \approx {1\over N} \sum_n \sum_{t=1}^T \nabla \log \pi(a_{n, t} | s_{n, t})  \underbrace{ \left( \sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \right) }_{ \text{reward to go} \; \hat Q_{n, t} } 
\]</div>
<h3 id="improvement-2-subtracting-a-reward-baseline"><code>Improvement 2</code>: subtracting a reward baseline</h3>
<ul>
<li>a simple baseline: mean return</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
\nabla J &amp; \approx {1\over N} \sum_n \nabla \log p \; [r(\tau) - b] \\
b &amp; := {1\over N} \sum_n r(\tau) \\
 \text{because} \; E [\nabla \log p \; b] &amp;= \int \underbrace{ p \nabla \log p }_{ \nabla p } \; b \; d\tau = b \nabla \underbrace{ \int p \; d\tau }_{ 1 } = 0
\end{align*}
\]</div>
<ul>
<li>optimal baseline</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\
\nabla J &amp;= E[ \underbrace{ \nabla \log p }_{g} \; (r - b) ] \\
\text{Var} &amp;= E[ (g (r-b))^2 ] - \underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\
\text{let} &amp; \; {d \text{Var} \over db} = 0 \\
 \implies &amp; {d \over db} \left( -2b E[g(\tau)^2 r(\tau)] + b^2 E[g(\tau)^2] \right) = 0 \\
\implies &amp; b = { E[g^2 r] \over E[g^2] }
\end{align*}
\]</div>
<h3 id="improvement-3-from-on-policy-to-off-policy-pg"><code>Improvement 3</code>: from on-policy to off-policy PG</h3>
<p><strong>the above result is on-policy —— need to generate new samples whenever policy neural net is updated</strong></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Importance_sampling">importance sampling</a>: learn about one distribution from another distribution</li>
</ul>
<div class="arithmatex">\[
E_{x\sim p(x)}[y] = \int p(x) \; y \; dx = \int q(x) {p(x) \over q(x)} \; y \; dx = E_{x\sim q(x)}[ {p(x) \over q(x)} y]
\]</div>
<ul>
<li><strong>didn't quite understand pages 24-26, conclusion</strong>:</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
\text{on-policy } &amp; \nabla_\theta J(\theta) \approx {1\over N} \sum_n \sum_t \nabla_\theta \log \pi_\theta (a_{n, t} | s_{n, t}) \hat Q_{n, t} \\
\text{off-policy } &amp; \nabla_\alpha J(\alpha) \approx {1\over N} \sum_n \sum_t {\pi_\alpha(a_{n, t} | s_{n, t}) \over \pi_\theta (a_{n, t} | s_{n, t}) } \nabla_\alpha \log \pi_\alpha (a_{n, t} | s_{n, t}) \hat Q_{n, t}
\end{align*}
\]</div>
<h3 id="improvement-4-natural-pg-rescaling-the-vanilla-pg"><code>Improvement 4</code>: Natural PG: <strong>rescaling</strong> the Vanilla PG</h3>
<p><img alt="" src="imgs/rl_natual_pg.png" /></p>
<ul>
<li><strong>didn't quite understand pages 35-36, conclusion</strong>:</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
\text{ Vanilla: } &amp; \theta \leftarrow \theta + \alpha \nabla J \\
\text{ Natural: } &amp; \theta \leftarrow \theta + \alpha F^{-1} \nabla J \\
\text{ where: } &amp; F = E_{\pi_\theta} [ (\nabla \log \pi) \; (\nabla \log \pi)^T ]
\end{align*}
\]</div>
<!--
$$
\begin{align*}
\theta &\leftarrow \theta + \alpha \nabla_\theta J(\theta) \\
\theta &\leftarrow \arg \max_{\theta'} (\theta' - \theta)^T \nabla_\theta J(\theta) \text{ s.t. } \lVert \theta' - \theta \rVert^2 \le \epsilon
\end{align*}
$$
-->

<h2 id="lecture-6-actor-critic">Lecture 6: Actor-Critic</h2>
<h3 id="improvement-5-actor-critic-still-trying-to-reduce-variance"><code>Improvement 5</code>: Actor-Critic: still trying to reduce variance</h3>
<div class="arithmatex">\[
\begin{align*}
\text{before: single trajectory } &amp; \nabla J \approx {1\over N} \sum_n \sum_t \nabla \log \pi ( \hat Q_{n, t} - b) \\
\text{AC: Exp over trajectories } &amp; \nabla J \approx {1\over N} \sum_n \sum_t \nabla \log \pi ( \underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \text{ advantage } } )
\end{align*}
\]</div>
<div class="arithmatex">\[
\begin{align*}
Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\
&amp;\approx r(s_t, a_t) + V(s_{t+1}) \\
\implies A(s_t, a_t) &amp;\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t)
\end{align*}
\]</div>
<ul>
<li>use neural net <span class="arithmatex">\(\phi\)</span> to approximate <span class="arithmatex">\(V\)</span> (supervised learning)</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
L_{\text{MSE}}(\phi) &amp;= {1\over 2} \sum_n \lVert V_\phi(s_n) - y_n \rVert^2 \\
y_{n, t} &amp;= r(s_{n, t}, a_{n, t}) + \underbrace{ \gamma \in [0, 1] }_{ \text{ discount } } \; \underbrace{ V_\phi(s_{n, \; t+1}) }_{ \text{ bootstrapping } } 
\end{align*}
\]</div>
<ul>
<li><strong>skip pages 14-15: discount factor for PG</strong></li>
</ul>
<h3 id="improvement-6-from-on-policy-to-off-policy-ac"><code>Improvement 6</code>: from on-policy to off-policy AC</h3>
<ul>
<li>on-policy AC</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{1. sample } (s, a, s', r) \\
&amp; \text{2. fit } V_\phi \text{ using } r + \gamma V_\phi(s') \\
&amp; \text{3. eval } A(s, a) = r(s, a) + \gamma V_\phi(s') - V_\phi(s) \\
&amp; \text{4. } \nabla J \approx \nabla \log \pi(a | s) A(s, a) \\
&amp; \text{5. } \theta \leftarrow \theta + \alpha \nabla J
\end{align*}
\]</div>
<ul>
<li>off-policy AC, can still use <strong>importance sampling</strong>, but use another method: $ V \to Q $<ul>
<li>cannot use the above directly because: <strong>$ a $ comes from old policies, not latest</strong></li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
&amp; \text{1. sample } (s, a, s', r), \text{ store in } R \\
&amp; \text{2. sample a batch } \{ s_n, a_n, s_n', r_n \} \text{ from } R \\
&amp; \text{3. fit } Q_\phi \text{ using } y_n = r_n + \gamma Q_\phi(s_n', a_n') \text{ for each } s_n, a_n \quad a_n' \sim \pi(a | s_n') \\
&amp; \text{4. } \nabla J \approx {1\over N} \sum_n \nabla \log \pi(a_n^\pi | s_n) Q(s_n, a_n^\pi) \quad a_n^\pi \sim \pi(a|s_n) \\
&amp; \text{5. } \theta \leftarrow \theta + \alpha \nabla J
\end{align*}
\]</div>
<p>page 26</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": ".", "features": [], "search": "assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="js/katex.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
    
  </body>
</html>