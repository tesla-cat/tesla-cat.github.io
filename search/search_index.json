{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"RL/","title":"Deep Reinforcement Learning","text":""},{"location":"RL/#summary-of-deep-reinforcement-learning","title":"Summary of Deep Reinforcement Learning","text":"<ul> <li>CS285 at UC Berkeley<ul> <li>videos</li> </ul> </li> </ul>"},{"location":"RL/#lecture-1-intro-and-overview","title":"Lecture 1: Intro and overview","text":""},{"location":"RL/#beyond-learning-from-reward","title":"Beyond learning from reward","text":"<ul> <li>basic RL: maximize rewards</li> <li>other methods:<ul> <li><code>Inverse RL</code>: learn reward function from example</li> <li><code>Transfer Learning</code>: transfer knowledge between domains</li> <li><code>Meta learning</code>: learning to learn</li> <li><code>Predicting</code>: use predictions to act</li> </ul> </li> </ul>"},{"location":"RL/#lecture-2-supervised-learning-of-behaviors","title":"Lecture 2: Supervised Learning of behaviors","text":"<ul> <li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li> <li>solution: generate examples of \"mistakes\" and their \"corrections\" (teach \"what didn't work and how to fix\", not \"what worked\")</li> </ul>"},{"location":"RL/#lecture-3-pytorch-tutorial","title":"Lecture 3: PyTorch tutorial","text":""},{"location":"RL/#lecture-4-intro-to-rl","title":"Lecture 4: Intro to RL","text":"<ul> <li>expanding the total reward over trajectory $ \\tau = (s_1, a_1, s_2, a_2 \\ldots) $:</li> </ul> \\[ \\begin{align*}     J &amp;= E_\\tau \\underbrace{ \\sum_t r(s_t, a_t) }_{ r(\\tau) }  \\\\     &amp;= E_{s_1} \\underbrace{         E_{a_1} \\underbrace{             r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \\ldots         }_{ Q(s_1, a_1) }     }_{V(s_1)}  \\end{align*} \\]"},{"location":"RL/#types-of-rl-algorithms","title":"Types of RL algorithms","text":"<ul> <li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li> <li><code>on policy</code>: once the policy is changed, need to generate new samples</li> </ul> <ul> <li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li> <li><code>Model based</code>: minimizes error of fit, but better model != better policy</li> <li><code>Policy Gradient</code>: gradient descent on true objective</li> </ul>"},{"location":"RL/#lecture-5-policy-gradients","title":"Lecture 5: Policy Gradients","text":"\\[ \\begin{align*} J(\\theta) &amp;= E_\\tau r(\\tau) = \\int p_\\theta(\\tau) \\; r(\\tau) \\; d\\tau  \\\\ \\nabla_\\theta J &amp;= \\int \\nabla p \\; r(\\tau) \\; d\\tau =  \\int p \\nabla \\log p \\; r(\\tau) \\; d\\tau  =  E_\\tau \\underbrace{ \\nabla \\log p }_{ \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) } \\; r(\\tau) \\\\ \\text{because} \\; p &amp;= p(s_1) \\prod_t \\pi_\\theta(a_t | s_t) \\; p(s_{t+1} | s_t, a_t) \\end{align*} \\] <p>notation $ \\nabla \\log \\pi(\\tau) := \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) $ </p> <ul> <li>approximate with sample mean:</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\left( \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t}) \\right)  \\left( \\sum_{t=1}^T r(s_{n, t}, a_{n, t}) \\right) \\]"},{"location":"RL/#improvement-1-reward-to-go","title":"<code>Improvement 1</code>: reward to go","text":"<ul> <li>causality: actions only affect the future, remove past rewards</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t})  \\underbrace{ \\left( \\sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \\right) }_{ \\text{reward to go} \\; \\hat Q_{n, t} }  \\]"},{"location":"RL/#improvement-2-subtracting-a-reward-baseline","title":"<code>Improvement 2</code>: subtracting a reward baseline","text":"<ul> <li>a simple baseline: mean return</li> </ul> \\[ \\begin{align*} \\nabla J &amp; \\approx {1\\over N} \\sum_n \\nabla \\log p \\; [r(\\tau) - b] \\\\ b &amp; := {1\\over N} \\sum_n r(\\tau) \\\\  \\text{because} \\; E [\\nabla \\log p \\; b] &amp;= \\int \\underbrace{ p \\nabla \\log p }_{ \\nabla p } \\; b \\; d\\tau = b \\nabla \\underbrace{ \\int p \\; d\\tau }_{ 1 } = 0 \\end{align*} \\] <ul> <li>optimal baseline</li> </ul> \\[ \\begin{align*} \\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\\\ \\nabla J &amp;= E[ \\underbrace{ \\nabla \\log p }_{g} \\; (r - b) ] \\\\ \\text{Var} &amp;= E[ (g (r-b))^2 ] - \\underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\\\ \\text{let} &amp; \\; {d \\text{Var} \\over db} = 0 \\\\  \\implies &amp; {d \\over db} \\left( -2b E[g(\\tau)^2 r(\\tau)] + b^2 E[g(\\tau)^2] \\right) = 0 \\\\ \\implies &amp; b = { E[g^2 r] \\over E[g^2] } \\end{align*} \\]"},{"location":"RL/#improvement-3-from-on-policy-to-off-policy-pg","title":"<code>Improvement 3</code>: from on-policy to off-policy PG","text":"<p>the above result is on-policy \u2014\u2014 need to generate new samples whenever policy neural net is updated</p> <ul> <li>importance sampling: learn about one distribution from another distribution</li> </ul> \\[ E_{x\\sim p(x)}[y] = \\int p(x) \\; y \\; dx = \\int q(x) {p(x) \\over q(x)} \\; y \\; dx = E_{x\\sim q(x)}[ {p(x) \\over q(x)} y] \\] <ul> <li>didn't quite understand pages 24-26, conclusion:</li> </ul> \\[ \\begin{align*} \\text{on-policy } &amp; \\nabla_\\theta J(\\theta) \\approx {1\\over N} \\sum_n \\sum_t \\nabla_\\theta \\log \\pi_\\theta (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\\\ \\text{off-policy } &amp; \\nabla_\\alpha J(\\alpha) \\approx {1\\over N} \\sum_n \\sum_t {\\pi_\\alpha(a_{n, t} | s_{n, t}) \\over \\pi_\\theta (a_{n, t} | s_{n, t}) } \\nabla_\\alpha \\log \\pi_\\alpha (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\end{align*} \\]"},{"location":"RL/#improvement-4-natural-pg-rescaling-the-vanilla-pg","title":"<code>Improvement 4</code>: Natural PG: rescaling the Vanilla PG","text":"<ul> <li>didn't quite understand pages 35-36, conclusion:</li> </ul> \\[ \\begin{align*} \\text{ Vanilla: } &amp; \\theta \\leftarrow \\theta + \\alpha \\nabla J \\\\ \\text{ Natural: } &amp; \\theta \\leftarrow \\theta + \\alpha F^{-1} \\nabla J \\\\ \\text{ where: } &amp; F = E_{\\pi_\\theta} [ (\\nabla \\log \\pi) \\; (\\nabla \\log \\pi)^T ] \\end{align*} \\]"},{"location":"RL/#lecture-6-actor-critic","title":"Lecture 6: Actor-Critic","text":""},{"location":"RL/#improvement-5-actor-critic-still-trying-to-reduce-variance","title":"<code>Improvement 5</code>: Actor-Critic: still trying to reduce variance","text":"\\[ \\begin{align*} \\text{before: single trajectory } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\hat Q_{n, t} - b) \\\\ \\text{AC: Exp over trajectories } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \\text{ advantage } } ) \\end{align*} \\] \\[ \\begin{align*} Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\\\ &amp;\\approx r(s_t, a_t) + V(s_{t+1}) \\\\ \\implies A(s_t, a_t) &amp;\\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t) \\end{align*} \\] <ul> <li>use neural net \\(\\phi\\) to approximate \\(V\\) (supervised learning)</li> </ul> \\[ \\begin{align*} L_{\\text{MSE}}(\\phi) &amp;= {1\\over 2} \\sum_n \\lVert V_\\phi(s_n) - y_n \\rVert^2 \\\\ y_{n, t} &amp;= r(s_{n, t}, a_{n, t}) + \\underbrace{ \\gamma \\in [0, 1] }_{ \\text{ discount } } \\; \\underbrace{ V_\\phi(s_{n, \\; t+1}) }_{ \\text{ bootstrapping } }  \\end{align*} \\] <ul> <li>skip pages 14-15: discount factor for PG</li> </ul>"},{"location":"RL/#improvement-6-from-on-policy-to-off-policy-ac","title":"<code>Improvement 6</code>: from on-policy to off-policy AC","text":"<ul> <li>on-policy AC</li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r) \\\\ &amp; \\text{2. fit } V_\\phi \\text{ using } r + \\gamma V_\\phi(s') \\\\ &amp; \\text{3. eval } A(s, a) = r(s, a) + \\gamma V_\\phi(s') - V_\\phi(s) \\\\ &amp; \\text{4. } \\nabla J \\approx \\nabla \\log \\pi(a | s) A(s, a) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <ul> <li>off-policy AC, can still use importance sampling, but use another method: $ V \\to Q $<ul> <li>cannot use the above directly because: $ a $ comes from old policies, not latest</li> </ul> </li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r), \\text{ store in } R \\\\ &amp; \\text{2. sample a batch } \\{ s_n, a_n, s_n', r_n \\} \\text{ from } R \\\\ &amp; \\text{3. fit } Q_\\phi \\text{ using } y_n = r_n + \\gamma Q_\\phi(s_n', a_n') \\text{ for each } s_n, a_n \\quad a_n' \\sim \\pi(a | s_n') \\\\ &amp; \\text{4. } \\nabla J \\approx {1\\over N} \\sum_n \\nabla \\log \\pi(a_n^\\pi | s_n) Q(s_n, a_n^\\pi) \\quad a_n^\\pi \\sim \\pi(a|s_n) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <p>page 26</p>"},{"location":"nanoDeepSeek/","title":"nanoDeepSeek","text":"<ul> <li>GitHub</li> <li>Full code for reference</li> </ul>"},{"location":"nanoDeepSeek/#1-token-word-embedding","title":"1. Token (word) Embedding","text":"<ul> <li> <p>torch.nn.functional.embedding</p> </li> <li> <p>In LLMs:</p> <ul> <li>Each token (word) is mapped to an integer</li> <li>This integer is then mapped to a vector</li> </ul> </li> <li>The code below is a module that maps integers (0 ~ 102400) to vectors (dim = 2048)</li> <li> <p>In this parallel version:</p> <ul> <li><code>world_size = number of GPUs</code></li> <li><code>rank = GPU index</code></li> </ul> </li> <li> <p>Given:  </p> <ul> <li>$ W \\in \\mathbb{R}^{V \\times d} $: Embedding matrix  </li> <li>$ x $: Input index  </li> </ul> </li> <li>Embedding function: $$ E(x) = W[x] $$</li> <li>Equivalently, using a one-hot vector $ e_x $:      $$ E(x) = W^T e_x $$</li> </ul> <pre><code># vocab_size: int = 102400\n# dim: int = 2048  # Model dimension.\n# s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n\nclass ParallelEmbedding(nn.Module):\n    def __init__(s, vocab_size, dim):\n        super().__init__()\n        assert vocab_size % world_size == 0\n        s.dx = vocab_size // world_size\n        s.weight = nn.Parameter(tc.empty(s.dx, dim))\n\n    def forward(s, x: tc.Tensor):\n        x1 = rank * s.dx\n        if world_size &gt; 1:\n            mask = (x &lt; x1) | (x &gt;= x1 + s.dx)\n            x -= x1\n            x[mask] = 0\n        y = F.embedding(x, s.weight)\n        if world_size &gt; 1:\n            y[mask] = 0\n            dist.all_reduce(y)  # default op: sum\n        return y\n</code></pre>"},{"location":"nanoDeepSeek/#2-linear-layers","title":"2. Linear Layers","text":"<ul> <li> <p>torch.nn.functional.linear</p> </li> <li> <p>Maps a vector to another vector: $$ y = xW^T + b $$</p> </li> <li> <p>The code uses <code>quantization</code> + <code>parallelism</code>, I am ignoring these for now</p> <ul> <li><code>tc.float32</code> element size: 4 bytes</li> <li><code>tc.int8</code> element size: 1 byte</li> </ul> </li> </ul> <pre><code>def linear(x, w: tc.Tensor, b=None) -&gt; tc.Tensor:\n    if w.element_size() &gt; 1:\n        return F.linear(x, w, b)\n    elif gemm_impl == \"bf16\":\n        w = weight_dequant(w, w.scale)\n        return F.linear(x, w, b)\n    else:\n        x, scale = act_quant(x, block_size)\n        y = fp8_gemm(x, scale, w, w.scale)\n        return y if b is None else y + b\n</code></pre> <pre><code>class Linear(nn.Module):\n    part_out_features: int\n    dtype = tc.bfloat16\n\n    def __init__(s, I, O, bias=False, dtype=None):\n        super().__init__()\n        s.weight = nn.Parameter(tc.empty(O, I, dtype=dtype or Linear.dtype))\n        if s.weight.element_size() == 1:\n            O2 = (O + block_size - 1) // block_size\n            I2 = (I + block_size - 1) // block_size\n            s.weight.scale = s.scale = nn.Parameter(tc.empty(O2, I2, dtype=tc.float32))\n        else:\n            s.register_parameter(\"scale\", None)\n        if bias:\n            s.bias = nn.Parameter(tc.empty(s.part_out_features))\n        else:\n            s.register_parameter(\"bias\", None)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class ColumnParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert O % world_size == 0\n        s.part_out_features = O // world_size\n        super().__init__(I, s.part_out_features, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class RowParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert I % world_size == 0\n        s.part_in_features = I // world_size\n        super().__init__(s.part_in_features, O, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        y = linear(x, s.weight)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return y if s.bias is None else y + s.bias\n</code></pre>"},{"location":"nanoDeepSeek/#3-rms-normalization","title":"3. RMS Normalization","text":"<ul> <li> <p>torch.nn.RMSNorm</p> </li> <li> <p>Scales a vector (in order to have stabler gradients)</p> </li> </ul> \\[ y = \\frac{x}{\\mathrm{RMS}(x)} \\cdot \\gamma \\\\[5pt] \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{N} \\sum_i x_i^2} \\\\ \\gamma: \\text{learnable parameter} \\] <pre><code>class RMSNorm(nn.Module):\n    def __init__(s, dim, eps=1e-6):\n        super().__init__()\n        s.dim, s.eps = dim, eps\n        s.weight = nn.Parameter(tc.ones(dim))\n\n    def forward(s, x: tc.Tensor):\n        return F.rms_norm(x, (s.dim,), s.weight, s.eps)\n</code></pre>"},{"location":"nanoDeepSeek/#4-rope-rotary-position-embedding","title":"4. RoPE: Rotary Position Embedding","text":"<ul> <li>This transformation treats neural network activations as complex numbers, it applies complex rotations, encodes position $ t $ into vectors: </li> </ul> \\[  z' = z \\cdot e^{i \\omega t} \\\\[5pt] \\omega = {1 \\over \\text{base}^{d / D} } \\] <ul> <li>This ensures that the dot product (attention) after PE only depends on relative position \\(t_1 - t_2\\):</li> </ul> \\[ \\text{Re}(z_1 \\cdot z_2^*)  = \\text{Re}( (x_1 + i y_1) (x_2 - i y_2) )  = x_1 x_2 + y_1 y_2 =: \\text{Dot}(z_1, z_2) \\] \\[ \\text{Dot}(q', k') = \\text{Re}(q' \\cdot k'^*)  = \\text{Re}(q e^{i \\omega t_1} \\cdot k^* e^{-i \\omega t_2})  = \\text{Re}(q \\cdot k^* e^{i \\omega (t_1 - t_2)})  \\] <ul> <li>Below is my simple implementation:</li> </ul> <pre><code>def simple_RoPE(x: tc.Tensor, base=10000.0):\n    B, T, H, D2 = x.shape  # batch, time, head, dim*2\n    D = D2 // 2\n\n    t = tc.arange(T)  # shape: T\n    w = 1.0 / (base ** (tc.arange(0, D, dtype=tc.float32) / D))  # shape: D\n    wt = tc.outer(t, w)  # shape: T, D\n    e_iwt = tc.polar(tc.ones_like(wt), wt).view(1, T, 1, D)\n    z = tc.view_as_complex(x.float().view(B, T, H, D, 2))  # shape: B, T, H, D\n    y = tc.view_as_real(z * e_iwt).view(B, T, H, D2)\n    return y.to(x.dtype)\n</code></pre> full version <pre><code>def precompute_freqs_cis(a: ModelArgs):\n    dim = a.qk_rope_head_dim\n    base = a.rope_theta\n\n    def find_correction_dim(num_rot, dim, base, max_T):\n        return dim * math.log(max_T / (num_rot * 2 * math.pi)) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_T):\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_T))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_T))\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        if min == max:\n            max += 0.001\n        linear_func = (tc.arange(dim, dtype=tc.float32) - min) / (max - min)\n        return tc.clamp(linear_func, 0, 1)\n\n    freqs = 1.0 / (base ** (tc.arange(0, dim, 2, dtype=tc.float32) / dim))\n    if a.max_seq_len &gt; a.original_seq_len:\n        low, high = find_correction_range(\n            a.beta_fast, a.beta_slow, dim, base, a.original_seq_len\n        )\n        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n        freqs = freqs / a.rope_factor * (1 - smooth) + freqs * smooth\n\n    t = tc.arange(a.max_seq_len)\n    freqs = tc.outer(t, freqs)\n    return tc.polar(tc.ones_like(freqs), freqs)\n\n\ndef apply_rotary_emb(x: tc.Tensor, freqs_cis: tc.Tensor):\n    dtype = x.dtype\n    x = tc.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n    return tc.view_as_real(x * freqs_cis).flatten(3).to(dtype)\n</code></pre>"},{"location":"nanoDeepSeek/#5-mla-multi-head-latent-attention","title":"5. MLA: Multi-head Latent Attention","text":"<ul> <li>Original Attention Mechanism<ul> <li>A weighted mixture of word meanings by combining the value vectors $ V $ using attention weights (similarity between queries $ Q $ and keys $ K $)</li> <li>$ n $: sequence length, $ d $: token embedding dim</li> </ul> </li> </ul> \\[ Q = X W_Q \\quad K = X W_K \\quad V = X W_V \\\\ A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y_\\text{MultiHead} = \\text{Concat}(A_1, ..., A_h) \\; W_O \\] object shape $ X $ $ (n, d) $ $ W_Q, W_K, W_V $ $ (d, d_k) \\quad (d, d_k) \\quad (d, d_v) $ $ Q, K, V $ $ (n, d_k) \\quad (n, d_k) \\quad (n, d_v) $ $ Q K^T \\quad A $ $ (n, n) \\quad (n, d_v) $ $ W_O \\quad y $ $ (h \\cdot d_v, d) \\quad (n, d) $ <ul> <li> <p>LoRA: Low-Rank Adaptation</p> <ul> <li>Decompose $ W^{m\\times n} = W_B^{m\\times r} \\cdot W_A^{r\\times n} $ where $ r \\ll \\min(m, n) $ is the rank</li> <li>To reduce the number of parameters</li> <li>Essentially a compression $ (W_A) $ and decompression $ (W_B) $</li> <li>Latent space: the vector space after compression</li> </ul> </li> <li> <p>MLA</p> </li> </ul> \\[ q = W_{qB} \\cdot \\text{RMSNorm}(W_{qA} \\cdot x) \\text{ if LoRA else } W_q \\cdot x \\\\ \\text{split: } q \\rightarrow q_{\\text{nope}}, q_{\\text{pe}} \\rightarrow q_{\\text{nope}}, \\text{RoPE}( q_{\\text{pe}} ) \\rightarrow q \\\\[10pt]  kv, k_{\\text{pe}} = W_{kvA} \\cdot x \\\\  k_{\\text{pe}} = \\text{RoPE}( k_{\\text{pe}} ) \\\\  k_{\\text{nope}}, v = W_{kvB} \\cdot \\text{RMSNorm}(kv) \\\\ \\text{concat: } k_{\\text{nope}}, k_{\\text{pe}} \\rightarrow k \\\\[10pt] A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y = W_O A \\] <p></p> <pre><code>class MLA(nn.Module):\n    k_cache: tc.Tensor\n    v_cache: tc.Tensor\n    kv_cache: tc.Tensor\n    pe_cache: tc.Tensor\n\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        s.n_local_heads = a.n_heads // world_size\n        s.qk_head_dim = a.qk_nope_head_dim + a.qk_rope_head_dim\n\n        if a.q_lora_rank == 0:\n            s.wq = ColumnParallelLinear(a.dim, a.n_heads * s.qk_head_dim)\n        else:\n            s.wq_a = Linear(a.dim, a.q_lora_rank)\n            s.q_norm = RMSNorm(a.q_lora_rank)\n            s.wq_b = ColumnParallelLinear(a.q_lora_rank, a.n_heads * s.qk_head_dim)\n        s.wkv_a = Linear(a.dim, a.kv_lora_rank + a.qk_rope_head_dim)\n        s.kv_norm = RMSNorm(a.kv_lora_rank)\n        s.wkv_b = ColumnParallelLinear(\n            a.kv_lora_rank, a.n_heads * (a.qk_nope_head_dim + a.v_head_dim)\n        )\n        s.wo = RowParallelLinear(a.n_heads * a.v_head_dim, a.dim)\n        s.softmax_scale = s.qk_head_dim**-0.5\n        if a.max_seq_len &gt; a.original_seq_len:\n            mscale = 0.1 * a.mscale * math.log(a.rope_factor) + 1.0\n            s.softmax_scale = s.softmax_scale * mscale * mscale\n\n        B, T, H = a.max_batch_size, a.max_seq_len, s.n_local_heads\n        persis = False\n        if attn_impl == \"naive\":\n            s.register_buffer(\"k_cache\", tc.zeros(B, T, H, s.qk_head_dim), persis)\n            s.register_buffer(\"v_cache\", tc.zeros(B, T, H, a.v_head_dim), persis)\n        else:\n            s.register_buffer(\"kv_cache\", tc.zeros(B, T, a.kv_lora_rank), persis)\n            s.register_buffer(\"pe_cache\", tc.zeros(B, T, a.qk_rope_head_dim), persis)\n\n    def forward(s, x: tc.Tensor, start_pos, freqs_cis, mask: tc.Tensor):\n        a = s.args\n        B, T, _ = x.size()\n        p1 = start_pos\n        p2 = p1 + T\n\n        if a.q_lora_rank == 0:\n            q: tc.Tensor = s.wq(x)\n        else:\n            q = s.wq_b(s.q_norm(s.wq_a(x)))\n\n        q = q.view(B, T, s.n_local_heads, s.qk_head_dim)\n        q_nope, q_pe = tc.split(q, [a.qk_nope_head_dim, a.qk_rope_head_dim], dim=-1)\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n\n        kv = s.wkv_a(x)\n        kv, k_pe = tc.split(kv, [a.kv_lora_rank, a.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n\n        if attn_impl == \"naive\":\n            q = tc.cat([q_nope, q_pe], dim=-1)\n            kv: tc.Tensor = s.wkv_b(s.kv_norm(kv))\n            kv = kv.view(B, T, s.n_local_heads, a.qk_nope_head_dim + a.v_head_dim)\n            k_nope, v = tc.split(kv, [a.qk_nope_head_dim, a.v_head_dim], dim=-1)\n            k = tc.cat([k_nope, k_pe.expand(-1, -1, s.n_local_heads, -1)], dim=-1)\n            s.k_cache[:B, p1:p2] = k\n            s.v_cache[:B, p1:p2] = v\n            scores: tc.Tensor = (\n                tc.einsum(\"bshd,bthd-&gt;bsht\", q, s.k_cache[:B, :p2]) * s.softmax_scale\n            )\n        else:\n            wkv_b = (\n                s.wkv_b.weight\n                if s.wkv_b.scale is None\n                else weight_dequant(s.wkv_b.weight, s.wkv_b.scale, block_size)\n            )\n            wkv_b = wkv_b.view(s.n_local_heads, -1, a.kv_lora_rank)\n            q_nope = tc.einsum(\"bshd,hdc-&gt;bshc\", q_nope, wkv_b[:, : a.qk_nope_head_dim])\n            s.kv_cache[:B, p1:p2] = s.kv_norm(kv)\n            s.pe_cache[:B, p1:p2] = k_pe.squeeze(2)\n            scores = (\n                tc.einsum(\"bshc,btc-&gt;bsht\", q_nope, s.kv_cache[:B, :p2])\n                + tc.einsum(\"bshr,btr-&gt;bsht\", q_pe, s.pe_cache[:B, :p2])\n            ) * s.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=tc.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = tc.einsum(\"bsht,bthd-&gt;bshd\", scores, s.v_cache[:B, :p2])\n        else:\n            x = tc.einsum(\"bsht,btc-&gt;bshc\", scores, s.kv_cache[:B, :p2])\n            x = tc.einsum(\"bshc,hdc-&gt;bshd\", x, wkv_b[:, -a.v_head_dim :])\n        x = s.wo(x.flatten(2))\n        return x\n</code></pre>"},{"location":"quant/brain_teasers/","title":"\u91cf\u5316\u9762\u8bd5\u9898: brain teasers","text":""},{"location":"quant/brain_teasers/#21-dynamic-programming","title":"2.1 Dynamic programming: \u5316\u7e41\u4e3a\u7b80","text":""},{"location":"quant/brain_teasers/#211","title":"2.1.1 \u6d77\u76d7\u5206\u91d1\u5e01","text":"<ul> <li>5\u4e2a\u6d77\u76d7\u5f97\u5230\u4e86100\u4e2a\u91d1\u5e01, \u4ed6\u4eec\u8981\u6295\u7968\u51fa\u4e00\u4e2a\u5206\u914d\u65b9\u6848</li> <li>\u7531\u7ea7\u522b\u6700\u9ad8\u7684\u4eba\u5148\u63d0\u51fa\u65b9\u6848<ul> <li>\u5982\u679c\u6709\u4e00\u534a\u53ca\u4ee5\u4e0a\u7684\u4eba\u8d5e\u540c, \u5219\u65b9\u6848\u843d\u5b9e</li> <li>\u5426\u5219\u8fd9\u4e2a\u4eba\u88ab\u6254\u8fdb\u6d77\u91cc, \u5269\u4e0b\u7684\u4eba\u4ee5\u6b64\u7c7b\u63a8\u7ee7\u7eed\u63d0\u51fa\u65b9\u6848\u5e76\u6295\u7968</li> </ul> </li> <li>\u6bcf\u4e2a\u6d77\u76d7\u4f18\u5148\u5e0c\u671b\u5f97\u5230\u6700\u591a\u91d1\u5e01, \u5176\u6b21\u5e0c\u671b\u4eba\u8d8a\u5c11\u8d8a\u597d\u3002     \u7ea7\u522b\u6700\u9ad8\u7684\u4eba\u5e94\u63d0\u51fa\u4ec0\u4e48\u65b9\u6848?</li> </ul>"},{"location":"quant/brain_teasers/#_1","title":"\u7b54\u6848:","text":"<ul> <li>\u5047\u5982\u67092\u4e2a\u4eba, 2\u4f1a\u63d0\u51fa 2(100) 1(0), 2 \u6295\u8d5e\u6210\u901a\u8fc7</li> <li>\u5047\u5982\u67093\u4e2a\u4eba, 3\u4f1a\u63d0\u51fa 3(99) 2(0) 1(1), 3 1 \u6295\u8d5e\u6210\u901a\u8fc7</li> <li>\u5047\u5982\u67094\u4e2a\u4eba, 4\u4f1a\u63d0\u51fa 4(99) 3(0) 2(1) 1(0), 4 2 \u6295\u8d5e\u6210\u901a\u8fc7</li> <li>\u5047\u5982\u67095\u4e2a\u4eba, 5\u4f1a\u63d0\u51fa 5(98) 4(0) 3(1) 2(0) 1(1), 5 3 1 \u6295\u8d5e\u6210\u901a\u8fc7</li> </ul>"},{"location":"quant/brain_teasers/#212","title":"2.1.2 \u8001\u864e\u5403\u7f8a","text":"<ul> <li>\u6709100\u53ea\u8001\u864e\u548c1\u53ea\u7f8a, \u8001\u864e\u5403\u4e86\u7f8a\u4ee5\u540e\u81ea\u5df1\u4f1a\u53d8\u6210\u4e00\u53ea\u7f8a, \u90a3\u4e48\u8fd9\u53ea\u7f8a\u4f1a\u88ab\u5403\u6389\u5417?</li> </ul>"},{"location":"quant/brain_teasers/#_2","title":"\u7b54\u6848:","text":"<ul> <li>\u5047\u5982\u67091\u53ea\u8001\u864e, \u5b83\u4f1a\u5403\u6389\u7f8a\u4e14\u4e0d\u4f1a\u88ab\u5403</li> <li>\u5047\u5982\u67092\u53ea\u8001\u864e, \u5982\u679c\u5176\u4e2d\u4e00\u53ea\u5403\u6389\u7f8a, \u5219\u4f1a\u53d8\u6210\u60c5\u5f621\u5e76\u88ab\u5403\u6389, \u6240\u4ee52\u53ea\u90fd\u4e0d\u5403</li> <li>\u5047\u5982\u67093\u53ea\u8001\u864e, \u5982\u679c\u5176\u4e2d\u4e00\u53ea\u5403\u6389\u7f8a, \u5219\u4f1a\u53d8\u6210\u60c5\u5f622\u4e14\u4e0d\u4f1a\u88ab\u5403</li> <li>\u6240\u4ee5\u5982\u679c\u6709\u5947\u6570\u53ea\u8001\u864e\u5219\u53ef\u4ee5\u5403, \u5076\u6570\u53ea\u5219\u4e0d\u5403</li> </ul>"},{"location":"quant/brain_teasers/#221","title":"2.2.1 \u591c\u95f4\u8fc7\u6cb3","text":"<ul> <li>4\u4e2a\u4eba\u8fc7\u6cb3\u65f6\u95f4\u5206\u522b\u4e3a A(10) B(5) C(2) D(1)</li> <li>1\u6b21\u53ea\u80fd\u8fc72\u4eba, \u53ea\u67091\u4e2a\u624b\u7535, \u6700\u77ed\u9700\u8981\u591a\u4e45?</li> </ul>"},{"location":"quant/brain_teasers/#_3","title":"\u7b54\u6848:","text":"<ul> <li>\u7b2c\u4e00\u53cd\u5e94, \u8ba9D\u5206\u522b\u5e26ABC\u8fc7\u6cb3\u5e76\u8fd4\u56de: 10 + 1 + 5 + 1 + 2 = 19</li> <li>\u601d\u8003: \u9700\u5c06 A(10) B(5) \u5408\u5e76</li> <li>CD\u8fc7, D\u8fd4\u56de, AB\u8fc7, C\u8fd4\u56de, CD\u8fc7: 2 + 1 + 10 + 2 + 2 = 17</li> </ul>"}]}