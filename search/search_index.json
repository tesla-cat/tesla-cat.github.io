{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"RL/","title":"Deep Reinforcement Learning","text":""},{"location":"RL/#summary-of-deep-reinforcement-learning","title":"Summary of Deep Reinforcement Learning","text":"<ul> <li>CS285 at UC Berkeley<ul> <li>videos</li> </ul> </li> </ul>"},{"location":"RL/#lecture-1-intro-and-overview","title":"Lecture 1: Intro and overview","text":""},{"location":"RL/#beyond-learning-from-reward","title":"Beyond learning from reward","text":"<ul> <li>basic RL: maximize rewards</li> <li>other methods:<ul> <li><code>Inverse RL</code>: learn reward function from example</li> <li><code>Transfer Learning</code>: transfer knowledge between domains</li> <li><code>Meta learning</code>: learning to learn</li> <li><code>Predicting</code>: use predictions to act</li> </ul> </li> </ul>"},{"location":"RL/#lecture-2-supervised-learning-of-behaviors","title":"Lecture 2: Supervised Learning of behaviors","text":"<ul> <li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li> <li>solution: generate examples of \"mistakes\" and their \"corrections\" (teach \"what didn't work and how to fix\", not \"what worked\")</li> </ul>"},{"location":"RL/#lecture-3-pytorch-tutorial","title":"Lecture 3: PyTorch tutorial","text":""},{"location":"RL/#lecture-4-intro-to-rl","title":"Lecture 4: Intro to RL","text":"<ul> <li>expanding the total reward over trajectory $ \\tau = (s_1, a_1, s_2, a_2 \\ldots) $:</li> </ul> \\[ \\begin{align*}     J &amp;= E_\\tau \\underbrace{ \\sum_t r(s_t, a_t) }_{ r(\\tau) }  \\\\     &amp;= E_{s_1} \\underbrace{         E_{a_1} \\underbrace{             r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \\ldots         }_{ Q(s_1, a_1) }     }_{V(s_1)}  \\end{align*} \\]"},{"location":"RL/#types-of-rl-algorithms","title":"Types of RL algorithms","text":"<ul> <li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li> <li><code>on policy</code>: once the policy is changed, need to generate new samples</li> </ul> <ul> <li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li> <li><code>Model based</code>: minimizes error of fit, but better model != better policy</li> <li><code>Policy Gradient</code>: gradient descent on true objective</li> </ul>"},{"location":"RL/#lecture-5-policy-gradients","title":"Lecture 5: Policy Gradients","text":"\\[ \\begin{align*} J(\\theta) &amp;= E_\\tau r(\\tau) = \\int p_\\theta(\\tau) \\; r(\\tau) \\; d\\tau  \\\\ \\nabla_\\theta J &amp;= \\int \\nabla p \\; r(\\tau) \\; d\\tau =  \\int p \\nabla \\log p \\; r(\\tau) \\; d\\tau  =  E_\\tau \\underbrace{ \\nabla \\log p }_{ \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) } \\; r(\\tau) \\\\ \\text{because} \\; p &amp;= p(s_1) \\prod_t \\pi_\\theta(a_t | s_t) \\; p(s_{t+1} | s_t, a_t) \\end{align*} \\] <p>notation $ \\nabla \\log \\pi(\\tau) := \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) $ </p> <ul> <li>approximate with sample mean:</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\left( \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t}) \\right)  \\left( \\sum_{t=1}^T r(s_{n, t}, a_{n, t}) \\right) \\]"},{"location":"RL/#improvement-1-reward-to-go","title":"<code>Improvement 1</code>: reward to go","text":"<ul> <li>causality: actions only affect the future, remove past rewards</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t})  \\underbrace{ \\left( \\sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \\right) }_{ \\text{reward to go} \\; \\hat Q_{n, t} }  \\]"},{"location":"RL/#improvement-2-subtracting-a-reward-baseline","title":"<code>Improvement 2</code>: subtracting a reward baseline","text":"<ul> <li>a simple baseline: mean return</li> </ul> \\[ \\begin{align*} \\nabla J &amp; \\approx {1\\over N} \\sum_n \\nabla \\log p \\; [r(\\tau) - b] \\\\ b &amp; := {1\\over N} \\sum_n r(\\tau) \\\\  \\text{because} \\; E [\\nabla \\log p \\; b] &amp;= \\int \\underbrace{ p \\nabla \\log p }_{ \\nabla p } \\; b \\; d\\tau = b \\nabla \\underbrace{ \\int p \\; d\\tau }_{ 1 } = 0 \\end{align*} \\] <ul> <li>optimal baseline</li> </ul> \\[ \\begin{align*} \\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\\\ \\nabla J &amp;= E[ \\underbrace{ \\nabla \\log p }_{g} \\; (r - b) ] \\\\ \\text{Var} &amp;= E[ (g (r-b))^2 ] - \\underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\\\ \\text{let} &amp; \\; {d \\text{Var} \\over db} = 0 \\\\  \\implies &amp; {d \\over db} \\left( -2b E[g(\\tau)^2 r(\\tau)] + b^2 E[g(\\tau)^2] \\right) = 0 \\\\ \\implies &amp; b = { E[g^2 r] \\over E[g^2] } \\end{align*} \\]"},{"location":"RL/#improvement-3-from-on-policy-to-off-policy-pg","title":"<code>Improvement 3</code>: from on-policy to off-policy PG","text":"<p>the above result is on-policy \u2014\u2014 need to generate new samples whenever policy neural net is updated</p> <ul> <li>importance sampling: learn about one distribution from another distribution</li> </ul> \\[ E_{x\\sim p(x)}[y] = \\int p(x) \\; y \\; dx = \\int q(x) {p(x) \\over q(x)} \\; y \\; dx = E_{x\\sim q(x)}[ {p(x) \\over q(x)} y] \\] <ul> <li>didn't quite understand pages 24-26, conclusion:</li> </ul> \\[ \\begin{align*} \\text{on-policy } &amp; \\nabla_\\theta J(\\theta) \\approx {1\\over N} \\sum_n \\sum_t \\nabla_\\theta \\log \\pi_\\theta (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\\\ \\text{off-policy } &amp; \\nabla_\\alpha J(\\alpha) \\approx {1\\over N} \\sum_n \\sum_t {\\pi_\\alpha(a_{n, t} | s_{n, t}) \\over \\pi_\\theta (a_{n, t} | s_{n, t}) } \\nabla_\\alpha \\log \\pi_\\alpha (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\end{align*} \\]"},{"location":"RL/#improvement-4-natural-pg-rescaling-the-vanilla-pg","title":"<code>Improvement 4</code>: Natural PG: rescaling the Vanilla PG","text":"<ul> <li>didn't quite understand pages 35-36, conclusion:</li> </ul> \\[ \\begin{align*} \\text{ Vanilla: } &amp; \\theta \\leftarrow \\theta + \\alpha \\nabla J \\\\ \\text{ Natural: } &amp; \\theta \\leftarrow \\theta + \\alpha F^{-1} \\nabla J \\\\ \\text{ where: } &amp; F = E_{\\pi_\\theta} [ (\\nabla \\log \\pi) \\; (\\nabla \\log \\pi)^T ] \\end{align*} \\]"},{"location":"RL/#lecture-6-actor-critic","title":"Lecture 6: Actor-Critic","text":""},{"location":"RL/#improvement-5-actor-critic-still-trying-to-reduce-variance","title":"<code>Improvement 5</code>: Actor-Critic: still trying to reduce variance","text":"\\[ \\begin{align*} \\text{before: single trajectory } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\hat Q_{n, t} - b) \\\\ \\text{AC: Exp over trajectories } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \\text{ advantage } } ) \\end{align*} \\] \\[ \\begin{align*} Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\\\ &amp;\\approx r(s_t, a_t) + V(s_{t+1}) \\\\ \\implies A(s_t, a_t) &amp;\\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t) \\end{align*} \\] <ul> <li>use neural net \\(\\phi\\) to approximate \\(V\\) (supervised learning)</li> </ul> \\[ \\begin{align*} L_{\\text{MSE}}(\\phi) &amp;= {1\\over 2} \\sum_n \\lVert V_\\phi(s_n) - y_n \\rVert^2 \\\\ y_{n, t} &amp;= r(s_{n, t}, a_{n, t}) + \\underbrace{ \\gamma \\in [0, 1] }_{ \\text{ discount } } \\; \\underbrace{ V_\\phi(s_{n, \\; t+1}) }_{ \\text{ bootstrapping } }  \\end{align*} \\] <ul> <li>skip pages 14-15: discount factor for PG</li> </ul>"},{"location":"RL/#improvement-6-from-on-policy-to-off-policy-ac","title":"<code>Improvement 6</code>: from on-policy to off-policy AC","text":"<ul> <li>on-policy AC</li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r) \\\\ &amp; \\text{2. fit } V_\\phi \\text{ using } r + \\gamma V_\\phi(s') \\\\ &amp; \\text{3. eval } A(s, a) = r(s, a) + \\gamma V_\\phi(s') - V_\\phi(s) \\\\ &amp; \\text{4. } \\nabla J \\approx \\nabla \\log \\pi(a | s) A(s, a) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <ul> <li>off-policy AC, can still use importance sampling, but use another method: $ V \\to Q $<ul> <li>cannot use the above directly because: $ a $ comes from old policies, not latest</li> </ul> </li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r), \\text{ store in } R \\\\ &amp; \\text{2. sample a batch } \\{ s_n, a_n, s_n', r_n \\} \\text{ from } R \\\\ &amp; \\text{3. fit } Q_\\phi \\text{ using } y_n = r_n + \\gamma Q_\\phi(s_n', a_n') \\text{ for each } s_n, a_n \\quad a_n' \\sim \\pi(a | s_n') \\\\ &amp; \\text{4. } \\nabla J \\approx {1\\over N} \\sum_n \\nabla \\log \\pi(a_n^\\pi | s_n) Q(s_n, a_n^\\pi) \\quad a_n^\\pi \\sim \\pi(a|s_n) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <p>page 26</p>"},{"location":"nanoDeepSeek/","title":"nanoDeepSeek","text":"<ul> <li>GitHub</li> <li>Full code for reference</li> </ul>"},{"location":"nanoDeepSeek/#1-token-word-embedding","title":"1. Token (word) Embedding","text":"<ul> <li> <p>torch.nn.functional.embedding</p> </li> <li> <p>In LLMs:</p> <ul> <li>Each token (word) is mapped to an integer</li> <li>This integer is then mapped to a vector</li> </ul> </li> <li>The code below is a module that maps integers (0 ~ 102400) to vectors (dim = 2048)</li> <li> <p>In this parallel version:</p> <ul> <li><code>world_size = number of GPUs</code></li> <li><code>rank = GPU index</code></li> </ul> </li> <li> <p>Given:  </p> <ul> <li>$ W \\in \\mathbb{R}^{V \\times d} $: Embedding matrix  </li> <li>$ x $: Input index  </li> </ul> </li> <li>Embedding function: $$ E(x) = W[x] $$</li> </ul> <pre><code># vocab_size: int = 102400\n# dim: int = 2048  # Model dimension.\n# s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n\nclass ParallelEmbedding(nn.Module):\n    def __init__(s, vocab_size, dim):\n        super().__init__()\n        assert vocab_size % world_size == 0\n        s.dx = vocab_size // world_size\n        s.weight = nn.Parameter(tc.empty(s.dx, dim))\n\n    def forward(s, x: tc.Tensor):\n        x1 = rank * s.dx\n        if world_size &gt; 1:\n            mask = (x &lt; x1) | (x &gt;= x1 + s.dx)\n            x -= x1\n            x[mask] = 0\n        y = F.embedding(x, s.weight)\n        if world_size &gt; 1:\n            y[mask] = 0\n            dist.all_reduce(y)  # default op: sum\n        return y\n</code></pre>"},{"location":"nanoDeepSeek/#2-linear-layers","title":"2. Linear Layers","text":"<ul> <li> <p>torch.nn.functional.linear</p> </li> <li> <p>Maps a vector to another vector: $$ y = xW^T + b $$</p> </li> <li> <p>The code uses <code>quantization</code> + <code>parallelism</code>, I am ignoring these for now</p> <ul> <li><code>tc.float32</code> element size: 4 bytes</li> <li><code>tc.int8</code> element size: 1 byte</li> </ul> </li> </ul> <pre><code>def linear(x, w: tc.Tensor, b=None) -&gt; tc.Tensor:\n    if w.element_size() &gt; 1:\n        return F.linear(x, w, b)\n    elif gemm_impl == \"bf16\":\n        w = weight_dequant(w, w.scale)\n        return F.linear(x, w, b)\n    else:\n        x, scale = act_quant(x, block_size)\n        y = fp8_gemm(x, scale, w, w.scale)\n        return y if b is None else y + b\n</code></pre> <pre><code>class Linear(nn.Module):\n    part_out_features: int\n    dtype = tc.bfloat16\n\n    def __init__(s, I, O, bias=False, dtype=None):\n        super().__init__()\n        s.weight = nn.Parameter(tc.empty(O, I, dtype=dtype or Linear.dtype))\n        if s.weight.element_size() == 1:\n            O2 = (O + block_size - 1) // block_size\n            I2 = (I + block_size - 1) // block_size\n            s.weight.scale = s.scale = nn.Parameter(tc.empty(O2, I2, dtype=tc.float32))\n        else:\n            s.register_parameter(\"scale\", None)\n        if bias:\n            s.bias = nn.Parameter(tc.empty(s.part_out_features))\n        else:\n            s.register_parameter(\"bias\", None)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class ColumnParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert O % world_size == 0\n        s.part_out_features = O // world_size\n        super().__init__(I, s.part_out_features, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class RowParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert I % world_size == 0\n        s.part_in_features = I // world_size\n        super().__init__(s.part_in_features, O, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        y = linear(x, s.weight)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return y if s.bias is None else y + s.bias\n</code></pre>"},{"location":"nanoDeepSeek/#3-rms-normalization","title":"3. RMS Normalization","text":"<ul> <li> <p>torch.nn.RMSNorm</p> </li> <li> <p>Scales a vector (in order to have stabler gradients)</p> </li> </ul> \\[ y = \\frac{x}{\\mathrm{RMS}(x)} \\cdot \\gamma \\\\[5pt] \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{N} \\sum_i x_i^2} \\\\ \\gamma: \\text{learnable parameter} \\] <pre><code>class RMSNorm(nn.Module):\n    def __init__(s, dim, eps=1e-6):\n        super().__init__()\n        s.dim, s.eps = dim, eps\n        s.weight = nn.Parameter(tc.ones(dim))\n\n    def forward(s, x: tc.Tensor):\n        return F.rms_norm(x, (s.dim,), s.weight, s.eps)\n</code></pre>"},{"location":"nanoDeepSeek/#4-rope-rotary-position-embedding","title":"4. RoPE: Rotary Position Embedding","text":"<ul> <li>This transformation treats neural network activations as complex numbers, it applies complex rotations, encodes position $ t $ into vectors: </li> </ul> \\[  z' = z \\cdot e^{i \\omega t} \\\\[5pt] \\omega = {1 \\over \\text{base}^{d / D} } \\] <ul> <li>This ensures that the dot product (attention) after PE only depends on relative position \\(t_1 - t_2\\):</li> </ul> \\[ \\text{Re}(z_1 \\cdot z_2^*)  = \\text{Re}( (x_1 + i y_1) (x_2 - i y_2) )  = x_1 x_2 + y_1 y_2 =: \\text{Dot}(z_1, z_2) \\] \\[ \\text{Dot}(q', k') = \\text{Re}(q' \\cdot k'^*)  = \\text{Re}(q e^{i \\omega t_1} \\cdot k^* e^{-i \\omega t_2})  = \\text{Re}(q \\cdot k^* e^{i \\omega (t_1 - t_2)})  \\] <ul> <li>Below is my simple implementation:</li> </ul> <pre><code>def simple_RoPE(x: tc.Tensor, base=10000.0):\n    B, T, H, D2 = x.shape  # batch, time, head, dim*2\n    D = D2 // 2\n\n    t = tc.arange(T)  # shape: T\n    w = 1.0 / (base ** (tc.arange(0, D, dtype=tc.float32) / D))  # shape: D\n    wt = tc.outer(t, w)  # shape: T, D\n    e_iwt = tc.polar(tc.ones_like(wt), wt).view(1, T, 1, D)\n    z = tc.view_as_complex(x.float().view(B, T, H, D, 2))  # shape: B, T, H, D\n    y = tc.view_as_real(z * e_iwt).view(B, T, H, D2)\n    return y.to(x.dtype)\n</code></pre> full version <pre><code>def precompute_freqs_cis(a: ModelArgs):\n    dim = a.qk_rope_head_dim\n    base = a.rope_theta\n\n    def find_correction_dim(num_rot, dim, base, max_T):\n        return dim * math.log(max_T / (num_rot * 2 * math.pi)) / (2 * math.log(base))\n\n    def find_correction_range(low_rot, high_rot, dim, base, max_T):\n        low = math.floor(find_correction_dim(low_rot, dim, base, max_T))\n        high = math.ceil(find_correction_dim(high_rot, dim, base, max_T))\n        return max(low, 0), min(high, dim - 1)\n\n    def linear_ramp_factor(min, max, dim):\n        if min == max:\n            max += 0.001\n        linear_func = (tc.arange(dim, dtype=tc.float32) - min) / (max - min)\n        return tc.clamp(linear_func, 0, 1)\n\n    freqs = 1.0 / (base ** (tc.arange(0, dim, 2, dtype=tc.float32) / dim))\n    if a.max_seq_len &gt; a.original_seq_len:\n        low, high = find_correction_range(\n            a.beta_fast, a.beta_slow, dim, base, a.original_seq_len\n        )\n        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n        freqs = freqs / a.rope_factor * (1 - smooth) + freqs * smooth\n\n    t = tc.arange(a.max_seq_len)\n    freqs = tc.outer(t, freqs)\n    return tc.polar(tc.ones_like(freqs), freqs)\n\n\ndef apply_rotary_emb(x: tc.Tensor, freqs_cis: tc.Tensor):\n    dtype = x.dtype\n    x = tc.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n    return tc.view_as_real(x * freqs_cis).flatten(3).to(dtype)\n</code></pre>"},{"location":"nanoDeepSeek/#5-mla-multi-head-latent-attention","title":"5. MLA: Multi-head Latent Attention","text":"<ul> <li>Original Attention Mechanism<ul> <li>A weighted mixture of word meanings by combining the value vectors $ V $ using attention weights (similarity between queries $ Q $ and keys $ K $)</li> <li>$ n $: sequence length, $ d $: token embedding dim</li> </ul> </li> </ul> \\[ Q = X W_Q \\quad K = X W_K \\quad V = X W_V \\\\ A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y_\\text{MultiHead} = \\text{Concat}(A_1, ..., A_h) \\; W_O \\] object shape $ X $ $ (n, d) $ $ W_Q, W_K, W_V $ $ (d, d_k) \\quad (d, d_k) \\quad (d, d_v) $ $ Q, K, V $ $ (n, d_k) \\quad (n, d_k) \\quad (n, d_v) $ $ Q K^T \\quad A $ $ (n, n) \\quad (n, d_v) $ $ W_O \\quad y $ $ (h \\cdot d_v, d) \\quad (n, d) $ <ul> <li> <p>LoRA: Low-Rank Adaptation</p> <ul> <li>Decompose $ W^{m\\times n} = W_B^{m\\times r} \\cdot W_A^{r\\times n} $ where $ r \\ll \\min(m, n) $ is the rank</li> <li>To reduce the number of parameters</li> <li>Essentially a compression $ (W_A) $ and decompression $ (W_B) $</li> <li>Latent space: the vector space after compression</li> </ul> </li> <li> <p>MLA</p> </li> </ul> \\[ q = W_{qB} \\cdot \\text{RMSNorm}(W_{qA} \\cdot x) \\text{ if LoRA else } W_q \\cdot x \\\\ \\text{split: } q \\rightarrow q_{\\text{nope}}, q_{\\text{pe}} \\rightarrow q_{\\text{nope}}, \\text{RoPE}( q_{\\text{pe}} ) \\rightarrow q \\\\[10pt]  kv, k_{\\text{pe}} = W_{kvA} \\cdot x \\\\  k_{\\text{pe}} = \\text{RoPE}( k_{\\text{pe}} ) \\\\  k_{\\text{nope}}, v = W_{kvB} \\cdot \\text{RMSNorm}(kv) \\\\ \\text{concat: } k_{\\text{nope}}, k_{\\text{pe}} \\rightarrow k \\\\[10pt] A = \\text{softmax} \\left( {Q K^T \\over \\sqrt{d_k} } \\right) V \\\\ y = W_O A \\] <p></p> <pre><code>class MLA(nn.Module):\n    k_cache: tc.Tensor\n    v_cache: tc.Tensor\n    kv_cache: tc.Tensor\n    pe_cache: tc.Tensor\n\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        s.n_local_heads = a.n_heads // world_size\n        s.qk_head_dim = a.qk_nope_head_dim + a.qk_rope_head_dim\n\n        if a.q_lora_rank == 0:\n            s.wq = ColumnParallelLinear(a.dim, a.n_heads * s.qk_head_dim)\n        else:\n            s.wq_a = Linear(a.dim, a.q_lora_rank)\n            s.q_norm = RMSNorm(a.q_lora_rank)\n            s.wq_b = ColumnParallelLinear(a.q_lora_rank, a.n_heads * s.qk_head_dim)\n        s.wkv_a = Linear(a.dim, a.kv_lora_rank + a.qk_rope_head_dim)\n        s.kv_norm = RMSNorm(a.kv_lora_rank)\n        s.wkv_b = ColumnParallelLinear(\n            a.kv_lora_rank, a.n_heads * (a.qk_nope_head_dim + a.v_head_dim)\n        )\n        s.wo = RowParallelLinear(a.n_heads * a.v_head_dim, a.dim)\n        s.softmax_scale = s.qk_head_dim**-0.5\n        if a.max_seq_len &gt; a.original_seq_len:\n            mscale = 0.1 * a.mscale * math.log(a.rope_factor) + 1.0\n            s.softmax_scale = s.softmax_scale * mscale * mscale\n\n        B, T, H = a.max_batch_size, a.max_seq_len, s.n_local_heads\n        persis = False\n        if attn_impl == \"naive\":\n            s.register_buffer(\"k_cache\", tc.zeros(B, T, H, s.qk_head_dim), persis)\n            s.register_buffer(\"v_cache\", tc.zeros(B, T, H, a.v_head_dim), persis)\n        else:\n            s.register_buffer(\"kv_cache\", tc.zeros(B, T, a.kv_lora_rank), persis)\n            s.register_buffer(\"pe_cache\", tc.zeros(B, T, a.qk_rope_head_dim), persis)\n\n    def forward(s, x: tc.Tensor, start_pos, freqs_cis, mask: tc.Tensor):\n        a = s.args\n        B, T, _ = x.size()\n        p1 = start_pos\n        p2 = p1 + T\n\n        if a.q_lora_rank == 0:\n            q: tc.Tensor = s.wq(x)\n        else:\n            q = s.wq_b(s.q_norm(s.wq_a(x)))\n\n        q = q.view(B, T, s.n_local_heads, s.qk_head_dim)\n        q_nope, q_pe = tc.split(q, [a.qk_nope_head_dim, a.qk_rope_head_dim], dim=-1)\n        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n\n        kv = s.wkv_a(x)\n        kv, k_pe = tc.split(kv, [a.kv_lora_rank, a.qk_rope_head_dim], dim=-1)\n        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n\n        if attn_impl == \"naive\":\n            q = tc.cat([q_nope, q_pe], dim=-1)\n            kv: tc.Tensor = s.wkv_b(s.kv_norm(kv))\n            kv = kv.view(B, T, s.n_local_heads, a.qk_nope_head_dim + a.v_head_dim)\n            k_nope, v = tc.split(kv, [a.qk_nope_head_dim, a.v_head_dim], dim=-1)\n            k = tc.cat([k_nope, k_pe.expand(-1, -1, s.n_local_heads, -1)], dim=-1)\n            s.k_cache[:B, p1:p2] = k\n            s.v_cache[:B, p1:p2] = v\n            scores: tc.Tensor = (\n                tc.einsum(\"bshd,bthd-&gt;bsht\", q, s.k_cache[:B, :p2]) * s.softmax_scale\n            )\n        else:\n            wkv_b = (\n                s.wkv_b.weight\n                if s.wkv_b.scale is None\n                else weight_dequant(s.wkv_b.weight, s.wkv_b.scale, block_size)\n            )\n            wkv_b = wkv_b.view(s.n_local_heads, -1, a.kv_lora_rank)\n            q_nope = tc.einsum(\"bshd,hdc-&gt;bshc\", q_nope, wkv_b[:, : a.qk_nope_head_dim])\n            s.kv_cache[:B, p1:p2] = s.kv_norm(kv)\n            s.pe_cache[:B, p1:p2] = k_pe.squeeze(2)\n            scores = (\n                tc.einsum(\"bshc,btc-&gt;bsht\", q_nope, s.kv_cache[:B, :p2])\n                + tc.einsum(\"bshr,btr-&gt;bsht\", q_pe, s.pe_cache[:B, :p2])\n            ) * s.softmax_scale\n        if mask is not None:\n            scores += mask.unsqueeze(1)\n        scores = scores.softmax(dim=-1, dtype=tc.float32).type_as(x)\n        if attn_impl == \"naive\":\n            x = tc.einsum(\"bsht,bthd-&gt;bshd\", scores, s.v_cache[:B, :p2])\n        else:\n            x = tc.einsum(\"bsht,btc-&gt;bshc\", scores, s.kv_cache[:B, :p2])\n            x = tc.einsum(\"bshc,hdc-&gt;bshd\", x, wkv_b[:, -a.v_head_dim :])\n        x = s.wo(x.flatten(2))\n        return x\n</code></pre>"},{"location":"nanoDeepSeek/#6-mlp-swiglu","title":"6. MLP (SwiGLU)","text":"<ul> <li>SiLU (Sigmoid Linear Unit) activation function.     Also called Swish function</li> </ul> \\[ \\text{SiLU}(x) = x \\cdot \\sigma(x) = { x \\over 1 + e^{-x} } \\] \\[ y = w_2( \\; \\text{SiLU}(w_1(x)) \\cdot w_3(x) \\; ) \\\\ w_i \\text{ : linear layers} \\] <ul> <li> <p>This is not a typical MLP.      It is a <code>SwiGLU</code> (Gated Linear Unit with Swish activation)</p> <ul> <li>gating mechanism (element-wise product) allows the model to selectively emphasize / suppress certain features. It outperform standard MLPs in many tasks</li> </ul> </li> <li> <p>This is used in DeepSeek as:</p> <ol> <li>Expert / Shared Experts</li> <li>Dense Feed-Forward Network (only used by the first transformer layer,     the rest use MoE: Mixture-of-Experts)</li> </ol> </li> </ul> <pre><code>class MLP(nn.Module):\n    def __init__(s, dim, inter_dim):\n        super().__init__()\n        s.w1 = ColumnParallelLinear(dim, inter_dim)\n        s.w2 = RowParallelLinear(inter_dim, dim)\n        s.w3 = ColumnParallelLinear(dim, inter_dim)\n\n    def forward(s, x):\n        return s.w2(F.silu(s.w1(x)) * s.w3(x))\n\nclass Expert(nn.Module):\n    def __init__(s, dim, inter_dim):\n        super().__init__()\n        s.w1 = Linear(dim, inter_dim)\n        s.w2 = Linear(inter_dim, dim)\n        s.w3 = Linear(dim, inter_dim)\n\n    def forward(s, x):\n        return s.w2(F.silu(s.w1(x)) * s.w3(x))\n\n# in MoE\n# s.experts.append(Expert(a.dim, a.moe_inter_dim) if s.i1 &lt;= i &lt; s.i2 else None)\n# s.shared_experts = MLP(a.dim, a.n_shared_experts * a.moe_inter_dim)\n\n# in Block\n# n_dense_layers: int = 1\n# s.ffn = MLP(a.dim, a.inter_dim) if layer_id &lt; a.n_dense_layers else MoE(a)\n</code></pre>"},{"location":"nanoDeepSeek/#7-moe-mixture-of-experts","title":"7. MoE: Mixture-of-Experts","text":"<ul> <li>Gate: expert selector for MoE.     Selects top-K experts (\"brain regions\") to use, to enhance efficiency</li> </ul> \\[ \\text{weights, indices} = \\text{TopK}(\\text{softmax}(\\text{Linear}(x))) \\\\[10pt] y = SE(x) + \\sum_i \\text{weights}_i \\cdot E_i (x) \\\\ SE \\text{ : shared experts} \\\\ E_i \\text{ : routed expert} \\] <pre><code>class Gate(nn.Module):\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        s.weight = nn.Parameter(tc.empty(a.n_routed_experts, a.dim))\n        s.bias = nn.Parameter(tc.empty(a.n_routed_experts)) if a.dim == 7168 else None\n\n    def forward(s, x: tc.Tensor):\n        a = s.args\n        scores = linear(x, s.weight)\n        if a.score_func == \"softmax\":\n            scores = scores.softmax(dim=-1, dtype=tc.float32)\n        else:\n            scores = scores.sigmoid()\n        original_scores = scores\n        if s.bias is not None:\n            scores = scores + s.bias\n        if a.n_expert_groups &gt; 1:\n            scores = scores.view(x.size(0), a.n_expert_groups, -1)\n            if s.bias is None:\n                group_scores = scores.amax(dim=-1)\n            else:\n                group_scores = scores.topk(2, dim=-1)[0].sum(dim=-1)\n            indices = group_scores.topk(a.n_limited_groups, dim=-1)[1]\n            mask = tc.zeros_like(scores[..., 0]).scatter_(1, indices, True)\n            scores = (scores * mask.unsqueeze(-1)).flatten(1)\n        indices = tc.topk(scores, a.n_activated_experts, dim=-1)[1]\n        weights = original_scores.gather(1, indices)\n        if a.score_func == \"sigmoid\":\n            weights /= weights.sum(dim=-1, keepdim=True)\n        weights *= a.route_scale\n        return weights.type_as(x), indices\n\nclass MoE(nn.Module):\n    def __init__(s, a: ModelArgs):\n        super().__init__()\n        s.args = a\n        assert a.n_routed_experts % world_size == 0\n        s.n_local_experts = a.n_routed_experts // world_size\n        s.i1 = rank * s.n_local_experts\n        s.i2 = s.i1 + s.n_local_experts\n        s.gate = Gate(a)\n        s.experts = nn.ModuleList()\n        for i in range(a.n_routed_experts):\n            s.experts.append(\n                Expert(a.dim, a.moe_inter_dim) if s.i1 &lt;= i &lt; s.i2 else None\n            )\n        s.shared_experts = MLP(a.dim, a.n_shared_experts * a.moe_inter_dim)\n\n    def forward(s, x: tc.Tensor):\n        a = s.args\n        shape = x.size()\n        x = x.view(-1, a.dim)\n        weights, indices = s.gate(x)\n        y = tc.zeros_like(x)\n        counts = tc.bincount(indices.flatten(), minlength=a.n_routed_experts).tolist()\n        for i in range(s.i1, s.i2):\n            if counts[i] == 0:\n                continue\n            expert = s.experts[i]\n            idx, top = tc.where(indices == i)\n            y[idx] += expert(x[idx]) * weights[idx, top, None]\n        z = s.shared_experts(x)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return (y + z).view(shape)\n</code></pre>"},{"location":"nanoDeepSeek/#8-transformer","title":"8. Transformer","text":"<ul> <li>simply combines the above modules</li> </ul> \\[ x \\text{ : input tokens} \\\\ x \\leftarrow \\text{Embedding}(x) \\\\[10pt] \\text{(transformer layers:)} \\\\ x \\leftarrow x + \\text{MLA}(\\text{RMSNorm}(x)) \\\\ x \\leftarrow x + \\text{FFN}(\\text{RMSNorm}(x)) \\\\ \\text{FFN : MLP or MoE} \\\\[10pt] y_\\text{logits} = \\text{Linear}(\\text{RMSNorm}(x)) \\] <pre><code>class Block(nn.Module):\n    def __init__(s, layer_id, a: ModelArgs):\n        super().__init__()\n        s.attn = MLA(a)\n        s.ffn = MLP(a.dim, a.inter_dim) if layer_id &lt; a.n_dense_layers else MoE(a)\n        s.attn_norm = RMSNorm(a.dim)\n        s.ffn_norm = RMSNorm(a.dim)\n\n    def forward(s, x: tc.Tensor, start_pos, freqs_cis, mask):\n        x = x + s.attn(s.attn_norm(x), start_pos, freqs_cis, mask)\n        x = x + s.ffn(s.ffn_norm(x))\n        return x\n\nclass Transformer(nn.Module):\n    freqs_cis: tc.Tensor\n\n    def __init__(s, a: ModelArgs):\n        global world_size, rank\n        world_size = dist.get_world_size() if dist.is_initialized() else 1\n        rank = dist.get_rank() if dist.is_initialized() else 0\n        Linear.dtype = tc.float8_e4m3fn if a.dtype == \"fp8\" else tc.bfloat16\n        super().__init__()\n        s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n        s.layers = tc.nn.ModuleList([Block(i, a) for i in range(a.n_layers)])\n        s.norm = RMSNorm(a.dim)\n        s.head = ColumnParallelLinear(a.dim, a.vocab_size, dtype=tc.get_default_dtype())\n        s.register_buffer(\"freqs_cis\", precompute_freqs_cis(a), persistent=False)\n\n    @tc.inference_mode()\n    def forward(s, tokens: tc.Tensor, start_pos=0):\n        T = tokens.size(1)\n        h = s.embed(tokens)\n        freqs_cis = s.freqs_cis[start_pos : start_pos + T]\n        mask = None\n        if T &gt; 1:\n            mask = tc.full((T, T), float(\"-inf\"), device=tokens.device).triu_(1)\n        for layer in s.layers:\n            h = layer(h, start_pos, freqs_cis, mask)\n        h = s.norm(h)[:, -1]\n        logits = s.head(h)\n        if world_size &gt; 1:\n            all_logits = [tc.empty_like(logits) for _ in range(world_size)]\n            dist.all_gather(all_logits, logits)\n            logits = tc.cat(all_logits, dim=-1)\n        return logits\n</code></pre>"}]}