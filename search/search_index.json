{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#summary-of-deep-reinforcement-learning","title":"Summary of Deep Reinforcement Learning","text":"<ul> <li>CS285 at UC Berkeley</li> </ul>"},{"location":"#lecture-1-intro-and-overview","title":"Lecture 1: Intro and overview","text":""},{"location":"#beyond-learning-from-reward","title":"Beyond learning from reward","text":"<ul> <li>basic RL: maximize rewards</li> <li>other methods:<ul> <li><code>Inverse RL</code>: learn reward function from example</li> <li><code>Transfer Learning</code>: transfer knowledge between domains</li> <li><code>Meta learning</code>: learning to learn</li> <li><code>Predicting</code>: use predictions to act</li> </ul> </li> </ul>"},{"location":"#lecture-2-supervised-learning-of-behaviors","title":"Lecture 2: Supervised Learning of behaviors","text":"<ul> <li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li> <li>solution: generate examples of \"mistakes\" and their \"corrections\" (teach \"what didn't work and how to fix\", not \"what worked\")</li> </ul>"},{"location":"#lecture-3-pytorch-tutorial","title":"Lecture 3: PyTorch tutorial","text":""},{"location":"#lecture-4-intro-to-rl","title":"Lecture 4: Intro to RL","text":"<ul> <li>expanding the total reward over trajectory $ \\tau = (s_1, a_1, s_2, a_2 \\ldots) $:</li> </ul> \\[ \\begin{align*}     J &amp;= E_\\tau \\underbrace{ \\sum_t r(s_t, a_t) }_{ r(\\tau) }  \\\\     &amp;= E_{s_1} \\underbrace{         E_{a_1} \\underbrace{             r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \\ldots         }_{ Q(s_1, a_1) }     }_{V(s_1)}  \\end{align*} \\]"},{"location":"#types-of-rl-algorithms","title":"Types of RL algorithms","text":"<ul> <li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li> <li><code>on policy</code>: once the policy is changed, need to generate new samples</li> </ul> <ul> <li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li> <li><code>Model based</code>: minimizes error of fit, but better model != better policy</li> <li><code>Policy Gradient</code>: gradient descent on true objective</li> </ul>"},{"location":"#lecture-5-policy-gradients","title":"Lecture 5: Policy Gradients","text":"\\[ \\begin{align*} J(\\theta) &amp;= E_\\tau r(\\tau) = \\int p_\\theta(\\tau) \\; r(\\tau) \\; d\\tau  \\\\ \\nabla_\\theta J &amp;= \\int \\nabla p \\; r(\\tau) \\; d\\tau =  \\int p \\nabla \\log p \\; r(\\tau) \\; d\\tau  =  E_\\tau \\underbrace{ \\nabla \\log p }_{ \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) } \\; r(\\tau) \\\\ \\text{because} \\; p &amp;= p(s_1) \\prod_t \\pi_\\theta(a_t | s_t) \\; p(s_{t+1} | s_t, a_t) \\end{align*} \\] <p>notation $ \\nabla \\log \\pi(\\tau) := \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) $ </p> <ul> <li>approximate with sample mean:</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\left( \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t}) \\right)  \\left( \\sum_{t=1}^T r(s_{n, t}, a_{n, t}) \\right) \\]"},{"location":"#improvement-1-reward-to-go","title":"<code>Improvement 1</code>: reward to go","text":"<ul> <li>causality: actions only affect the future, remove past rewards</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t})  \\underbrace{ \\left( \\sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \\right) }_{ \\text{reward to go} \\; \\hat Q_{n, t} }  \\]"},{"location":"#improvement-2-subtracting-a-reward-baseline","title":"<code>Improvement 2</code>: subtracting a reward baseline","text":"<ul> <li>a simple baseline: mean return</li> </ul> \\[ \\begin{align*} \\nabla J &amp; \\approx {1\\over N} \\sum_n \\nabla \\log p \\; [r(\\tau) - b] \\\\ b &amp; := {1\\over N} \\sum_n r(\\tau) \\\\  \\text{because} \\; E [\\nabla \\log p \\; b] &amp;= \\int \\underbrace{ p \\nabla \\log p }_{ \\nabla p } \\; b \\; d\\tau = b \\nabla \\underbrace{ \\int p \\; d\\tau }_{ 1 } = 0 \\end{align*} \\] <ul> <li>optimal baseline</li> </ul> \\[ \\begin{align*} \\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\\\ \\nabla J &amp;= E[ \\underbrace{ \\nabla \\log p }_{g} \\; (r - b) ] \\\\ \\text{Var} &amp;= E[ (g (r-b))^2 ] - \\underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\\\ \\text{let} &amp; \\; {d \\text{Var} \\over db} = 0 \\\\  \\implies &amp; {d \\over db} \\left( -2b E[g(\\tau)^2 r(\\tau)] + b^2 E[g(\\tau)^2] \\right) = 0 \\\\ \\implies &amp; b = { E[g^2 r] \\over E[g^2] } \\end{align*} \\]"},{"location":"#improvement-3-from-on-policy-to-off-policy","title":"<code>Improvement 3</code>: from on-policy to off-policy","text":"<p>the above result is on-policy \u2014\u2014 need to generate new samples whenever policy neural net is updated</p> <ul> <li>importance sampling: learn about one distribution from another distribution</li> </ul> \\[ E_{x\\sim p(x)}[y] = \\int p(x) \\; y \\; dx = \\int q(x) {p(x) \\over q(x)} \\; y \\; dx = E_{x\\sim q(x)}[ {p(x) \\over q(x)} y] \\] <ul> <li>didn't quite understand pages 24-26, conclusion:</li> </ul> \\[ \\begin{align*} \\text{on-policy } &amp; \\nabla_\\theta J(\\theta) \\approx {1\\over N} \\sum_n \\sum_t \\nabla_\\theta \\log \\pi_\\theta (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\\\ \\text{off-policy } &amp; \\nabla_\\alpha J(\\alpha) \\approx {1\\over N} \\sum_n \\sum_t {\\pi_\\alpha(a_{n, t} | s_{n, t}) \\over \\pi_\\theta (a_{n, t} | s_{n, t}) } \\nabla_\\alpha \\log \\pi_\\alpha (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\end{align*} \\]"},{"location":"#improvement-4-natural-pg-rescaling-the-vanilla-pg","title":"<code>Improvement 4</code>: Natural PG: rescaling the Vanilla PG","text":"<ul> <li>didn't quite understand pages 35-36, conclusion:</li> </ul> \\[ \\begin{align*} \\text{ Vanilla: } &amp; \\theta \\leftarrow \\theta + \\alpha \\nabla J \\\\ \\text{ Natural: } &amp; \\theta \\leftarrow \\theta + \\alpha F^{-1} \\nabla J \\\\ \\text{ where: } &amp; F = E_{\\pi_\\theta} [ (\\nabla \\log \\pi) \\; (\\nabla \\log \\pi)^T ] \\end{align*} \\]"},{"location":"#lecture-6-actor-critic","title":"Lecture 6: Actor-Critic","text":""},{"location":"#improvement-5-actor-critic-still-trying-to-reduce-variance","title":"<code>Improvement 5</code>: Actor-Critic: still trying to reduce variance","text":"\\[ \\begin{align*} \\text{before: single trajectory } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\hat Q_{n, t} - b) \\\\ \\text{AC: Exp over trajectories } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \\text{ advantage } } ) \\end{align*} \\] \\[ \\begin{align*} Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\\\ &amp;\\approx r(s_t, a_t) + V(s_{t+1}) \\\\ \\implies A(s_t, a_t) &amp;\\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t) \\end{align*} \\] <p>page 7</p>"}]}