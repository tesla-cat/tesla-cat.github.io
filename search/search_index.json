{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"RL/","title":"Deep Reinforcement Learning","text":""},{"location":"RL/#summary-of-deep-reinforcement-learning","title":"Summary of Deep Reinforcement Learning","text":"<ul> <li>CS285 at UC Berkeley<ul> <li>videos</li> </ul> </li> </ul>"},{"location":"RL/#lecture-1-intro-and-overview","title":"Lecture 1: Intro and overview","text":""},{"location":"RL/#beyond-learning-from-reward","title":"Beyond learning from reward","text":"<ul> <li>basic RL: maximize rewards</li> <li>other methods:<ul> <li><code>Inverse RL</code>: learn reward function from example</li> <li><code>Transfer Learning</code>: transfer knowledge between domains</li> <li><code>Meta learning</code>: learning to learn</li> <li><code>Predicting</code>: use predictions to act</li> </ul> </li> </ul>"},{"location":"RL/#lecture-2-supervised-learning-of-behaviors","title":"Lecture 2: Supervised Learning of behaviors","text":"<ul> <li>small deviations accumulate to very different trajectories and states compared with training data (not Markovian)</li> <li>solution: generate examples of \"mistakes\" and their \"corrections\" (teach \"what didn't work and how to fix\", not \"what worked\")</li> </ul>"},{"location":"RL/#lecture-3-pytorch-tutorial","title":"Lecture 3: PyTorch tutorial","text":""},{"location":"RL/#lecture-4-intro-to-rl","title":"Lecture 4: Intro to RL","text":"<ul> <li>expanding the total reward over trajectory $ \\tau = (s_1, a_1, s_2, a_2 \\ldots) $:</li> </ul> \\[ \\begin{align*}     J &amp;= E_\\tau \\underbrace{ \\sum_t r(s_t, a_t) }_{ r(\\tau) }  \\\\     &amp;= E_{s_1} \\underbrace{         E_{a_1} \\underbrace{             r(s_1, a_1) + E_{s_2} E_{a_2} r(s_2, a_2) + \\ldots         }_{ Q(s_1, a_1) }     }_{V(s_1)}  \\end{align*} \\]"},{"location":"RL/#types-of-rl-algorithms","title":"Types of RL algorithms","text":"<ul> <li><code>off policy</code>: able to improve the policy without generating new samples from that policy</li> <li><code>on policy</code>: once the policy is changed, need to generate new samples</li> </ul> <ul> <li><code>Value function fitting</code>: At best, minimizes error of fit (Bellman error), not the same as expected reward; At worst, doesn't optimize anything.</li> <li><code>Model based</code>: minimizes error of fit, but better model != better policy</li> <li><code>Policy Gradient</code>: gradient descent on true objective</li> </ul>"},{"location":"RL/#lecture-5-policy-gradients","title":"Lecture 5: Policy Gradients","text":"\\[ \\begin{align*} J(\\theta) &amp;= E_\\tau r(\\tau) = \\int p_\\theta(\\tau) \\; r(\\tau) \\; d\\tau  \\\\ \\nabla_\\theta J &amp;= \\int \\nabla p \\; r(\\tau) \\; d\\tau =  \\int p \\nabla \\log p \\; r(\\tau) \\; d\\tau  =  E_\\tau \\underbrace{ \\nabla \\log p }_{ \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) } \\; r(\\tau) \\\\ \\text{because} \\; p &amp;= p(s_1) \\prod_t \\pi_\\theta(a_t | s_t) \\; p(s_{t+1} | s_t, a_t) \\end{align*} \\] <p>notation $ \\nabla \\log \\pi(\\tau) := \\sum_t \\nabla \\log \\pi_\\theta(a_t | s_t) $ </p> <ul> <li>approximate with sample mean:</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\left( \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t}) \\right)  \\left( \\sum_{t=1}^T r(s_{n, t}, a_{n, t}) \\right) \\]"},{"location":"RL/#improvement-1-reward-to-go","title":"<code>Improvement 1</code>: reward to go","text":"<ul> <li>causality: actions only affect the future, remove past rewards</li> </ul> \\[ \\nabla J \\approx {1\\over N} \\sum_n \\sum_{t=1}^T \\nabla \\log \\pi(a_{n, t} | s_{n, t})  \\underbrace{ \\left( \\sum_{t'=t}^T r(s_{n, t'}, a_{n, t'}) \\right) }_{ \\text{reward to go} \\; \\hat Q_{n, t} }  \\]"},{"location":"RL/#improvement-2-subtracting-a-reward-baseline","title":"<code>Improvement 2</code>: subtracting a reward baseline","text":"<ul> <li>a simple baseline: mean return</li> </ul> \\[ \\begin{align*} \\nabla J &amp; \\approx {1\\over N} \\sum_n \\nabla \\log p \\; [r(\\tau) - b] \\\\ b &amp; := {1\\over N} \\sum_n r(\\tau) \\\\  \\text{because} \\; E [\\nabla \\log p \\; b] &amp;= \\int \\underbrace{ p \\nabla \\log p }_{ \\nabla p } \\; b \\; d\\tau = b \\nabla \\underbrace{ \\int p \\; d\\tau }_{ 1 } = 0 \\end{align*} \\] <ul> <li>optimal baseline</li> </ul> \\[ \\begin{align*} \\text{Var} [x] &amp;= E[x^2] - E[x]^2 \\\\ \\nabla J &amp;= E[ \\underbrace{ \\nabla \\log p }_{g} \\; (r - b) ] \\\\ \\text{Var} &amp;= E[ (g (r-b))^2 ] - \\underbrace{ E[ g (r - b) ]^2 }_{ = E[ g r ]^2 } \\\\ \\text{let} &amp; \\; {d \\text{Var} \\over db} = 0 \\\\  \\implies &amp; {d \\over db} \\left( -2b E[g(\\tau)^2 r(\\tau)] + b^2 E[g(\\tau)^2] \\right) = 0 \\\\ \\implies &amp; b = { E[g^2 r] \\over E[g^2] } \\end{align*} \\]"},{"location":"RL/#improvement-3-from-on-policy-to-off-policy-pg","title":"<code>Improvement 3</code>: from on-policy to off-policy PG","text":"<p>the above result is on-policy \u2014\u2014 need to generate new samples whenever policy neural net is updated</p> <ul> <li>importance sampling: learn about one distribution from another distribution</li> </ul> \\[ E_{x\\sim p(x)}[y] = \\int p(x) \\; y \\; dx = \\int q(x) {p(x) \\over q(x)} \\; y \\; dx = E_{x\\sim q(x)}[ {p(x) \\over q(x)} y] \\] <ul> <li>didn't quite understand pages 24-26, conclusion:</li> </ul> \\[ \\begin{align*} \\text{on-policy } &amp; \\nabla_\\theta J(\\theta) \\approx {1\\over N} \\sum_n \\sum_t \\nabla_\\theta \\log \\pi_\\theta (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\\\ \\text{off-policy } &amp; \\nabla_\\alpha J(\\alpha) \\approx {1\\over N} \\sum_n \\sum_t {\\pi_\\alpha(a_{n, t} | s_{n, t}) \\over \\pi_\\theta (a_{n, t} | s_{n, t}) } \\nabla_\\alpha \\log \\pi_\\alpha (a_{n, t} | s_{n, t}) \\hat Q_{n, t} \\end{align*} \\]"},{"location":"RL/#improvement-4-natural-pg-rescaling-the-vanilla-pg","title":"<code>Improvement 4</code>: Natural PG: rescaling the Vanilla PG","text":"<ul> <li>didn't quite understand pages 35-36, conclusion:</li> </ul> \\[ \\begin{align*} \\text{ Vanilla: } &amp; \\theta \\leftarrow \\theta + \\alpha \\nabla J \\\\ \\text{ Natural: } &amp; \\theta \\leftarrow \\theta + \\alpha F^{-1} \\nabla J \\\\ \\text{ where: } &amp; F = E_{\\pi_\\theta} [ (\\nabla \\log \\pi) \\; (\\nabla \\log \\pi)^T ] \\end{align*} \\]"},{"location":"RL/#lecture-6-actor-critic","title":"Lecture 6: Actor-Critic","text":""},{"location":"RL/#improvement-5-actor-critic-still-trying-to-reduce-variance","title":"<code>Improvement 5</code>: Actor-Critic: still trying to reduce variance","text":"\\[ \\begin{align*} \\text{before: single trajectory } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\hat Q_{n, t} - b) \\\\ \\text{AC: Exp over trajectories } &amp; \\nabla J \\approx {1\\over N} \\sum_n \\sum_t \\nabla \\log \\pi ( \\underbrace{ Q(s_{n, t}, a_{n, t}) - V(s_{n, t}) }_{ A: \\text{ advantage } } ) \\end{align*} \\] \\[ \\begin{align*} Q(s_t, a_t) &amp;= r(s_t, a_t) + E_{ s_{t+1} } [V(s_{t+1})] \\\\ &amp;\\approx r(s_t, a_t) + V(s_{t+1}) \\\\ \\implies A(s_t, a_t) &amp;\\approx r(s_t, a_t) + V(s_{t+1}) - V(s_t) \\end{align*} \\] <ul> <li>use neural net \\(\\phi\\) to approximate \\(V\\) (supervised learning)</li> </ul> \\[ \\begin{align*} L_{\\text{MSE}}(\\phi) &amp;= {1\\over 2} \\sum_n \\lVert V_\\phi(s_n) - y_n \\rVert^2 \\\\ y_{n, t} &amp;= r(s_{n, t}, a_{n, t}) + \\underbrace{ \\gamma \\in [0, 1] }_{ \\text{ discount } } \\; \\underbrace{ V_\\phi(s_{n, \\; t+1}) }_{ \\text{ bootstrapping } }  \\end{align*} \\] <ul> <li>skip pages 14-15: discount factor for PG</li> </ul>"},{"location":"RL/#improvement-6-from-on-policy-to-off-policy-ac","title":"<code>Improvement 6</code>: from on-policy to off-policy AC","text":"<ul> <li>on-policy AC</li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r) \\\\ &amp; \\text{2. fit } V_\\phi \\text{ using } r + \\gamma V_\\phi(s') \\\\ &amp; \\text{3. eval } A(s, a) = r(s, a) + \\gamma V_\\phi(s') - V_\\phi(s) \\\\ &amp; \\text{4. } \\nabla J \\approx \\nabla \\log \\pi(a | s) A(s, a) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <ul> <li>off-policy AC, can still use importance sampling, but use another method: $ V \\to Q $<ul> <li>cannot use the above directly because: $ a $ comes from old policies, not latest</li> </ul> </li> </ul> \\[ \\begin{align*} &amp; \\text{1. sample } (s, a, s', r), \\text{ store in } R \\\\ &amp; \\text{2. sample a batch } \\{ s_n, a_n, s_n', r_n \\} \\text{ from } R \\\\ &amp; \\text{3. fit } Q_\\phi \\text{ using } y_n = r_n + \\gamma Q_\\phi(s_n', a_n') \\text{ for each } s_n, a_n \\quad a_n' \\sim \\pi(a | s_n') \\\\ &amp; \\text{4. } \\nabla J \\approx {1\\over N} \\sum_n \\nabla \\log \\pi(a_n^\\pi | s_n) Q(s_n, a_n^\\pi) \\quad a_n^\\pi \\sim \\pi(a|s_n) \\\\ &amp; \\text{5. } \\theta \\leftarrow \\theta + \\alpha \\nabla J \\end{align*} \\] <p>page 26</p>"},{"location":"nanoDeepSeek/","title":"nanoDeepSeek","text":"<ul> <li>GitHub</li> <li>Full code for reference</li> </ul>"},{"location":"nanoDeepSeek/#1-token-word-embedding","title":"1. Token (word) Embedding","text":"<ul> <li> <p>torch.nn.functional.embedding</p> </li> <li> <p>In LLMs:</p> <ul> <li>Each token (word) is mapped to an integer</li> <li>This integer is then mapped to a vector</li> </ul> </li> <li>The code below is a module that maps integers (0 ~ 102400) to vectors (dim = 2048)</li> <li> <p>In this parallel version:</p> <ul> <li><code>world_size = number of GPUs</code></li> <li><code>rank = GPU index</code></li> </ul> </li> <li> <p>Given:  </p> <ul> <li>$ W \\in \\mathbb{R}^{V \\times d} $: Embedding matrix  </li> <li>$ x $: Input index  </li> </ul> </li> <li>Embedding function: $$ E(x) = W[x] $$</li> <li>Equivalently, using a one-hot vector $ e_x $:      $$ E(x) = W^T e_x $$</li> </ul> <pre><code># vocab_size: int = 102400\n# dim: int = 2048  # Model dimension.\n# s.embed = ParallelEmbedding(a.vocab_size, a.dim)\n\nclass ParallelEmbedding(nn.Module):\n    def __init__(s, vocab_size, dim):\n        super().__init__()\n        assert vocab_size % world_size == 0\n        s.dx = vocab_size // world_size\n        s.weight = nn.Parameter(tc.empty(s.dx, dim))\n\n    def forward(s, x: tc.Tensor):\n        x1 = rank * s.dx\n        if world_size &gt; 1:\n            mask = (x &lt; x1) | (x &gt;= x1 + s.dx)\n            x -= x1\n            x[mask] = 0\n        y = F.embedding(x, s.weight)\n        if world_size &gt; 1:\n            y[mask] = 0\n            dist.all_reduce(y)  # default op: sum\n        return y\n</code></pre>"},{"location":"nanoDeepSeek/#2-linear-layers","title":"2. Linear Layers","text":"<ul> <li> <p>torch.nn.functional.linear</p> </li> <li> <p>Maps a vector to another vector: $$ y = xW^T + b $$</p> </li> <li> <p>The code uses <code>quantization</code> + <code>parallelism</code>, I am ignoring these for now</p> <ul> <li><code>tc.float32</code> element size: 4 bytes</li> <li><code>tc.int8</code> element size: 1 byte</li> </ul> </li> </ul> <pre><code>def linear(x, w: tc.Tensor, b=None) -&gt; tc.Tensor:\n    if w.element_size() &gt; 1:\n        return F.linear(x, w, b)\n    elif gemm_impl == \"bf16\":\n        w = weight_dequant(w, w.scale)\n        return F.linear(x, w, b)\n    else:\n        x, scale = act_quant(x, block_size)\n        y = fp8_gemm(x, scale, w, w.scale)\n        return y if b is None else y + b\n</code></pre> <pre><code>class Linear(nn.Module):\n    part_out_features: int\n    dtype = tc.bfloat16\n\n    def __init__(s, I, O, bias=False, dtype=None):\n        super().__init__()\n        s.weight = nn.Parameter(tc.empty(O, I, dtype=dtype or Linear.dtype))\n        if s.weight.element_size() == 1:\n            O2 = (O + block_size - 1) // block_size\n            I2 = (I + block_size - 1) // block_size\n            s.weight.scale = s.scale = nn.Parameter(tc.empty(O2, I2, dtype=tc.float32))\n        else:\n            s.register_parameter(\"scale\", None)\n        if bias:\n            s.bias = nn.Parameter(tc.empty(s.part_out_features))\n        else:\n            s.register_parameter(\"bias\", None)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class ColumnParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert O % world_size == 0\n        s.part_out_features = O // world_size\n        super().__init__(I, s.part_out_features, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        return linear(x, s.weight, s.bias)\n</code></pre> <pre><code>class RowParallelLinear(Linear):\n    def __init__(s, I, O, bias=False, dtype=None):\n        assert I % world_size == 0\n        s.part_in_features = I // world_size\n        super().__init__(s.part_in_features, O, bias, dtype)\n\n    def forward(s, x: tc.Tensor):\n        y = linear(x, s.weight)\n        if world_size &gt; 1:\n            dist.all_reduce(y)\n        return y if s.bias is None else y + s.bias\n</code></pre>"},{"location":"nanoDeepSeek/#3-rms-normalization","title":"3. RMS Normalization","text":"<ul> <li> <p>torch.nn.RMSNorm</p> </li> <li> <p>Scales a vector (in order to have stabler gradients)</p> </li> </ul> \\[ y = \\frac{x}{\\mathrm{RMS}(x)} \\cdot \\gamma \\\\[5pt] \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{N} \\sum_i x_i^2} \\\\ \\gamma: \\text{learnable parameter} \\] <pre><code>class RMSNorm(nn.Module):\n    def __init__(s, dim, eps=1e-6):\n        super().__init__()\n        s.dim, s.eps = dim, eps\n        s.weight = nn.Parameter(tc.ones(dim))\n\n    def forward(s, x: tc.Tensor):\n        return F.rms_norm(x, (s.dim,), s.weight, s.eps)\n</code></pre>"},{"location":"nanoDeepSeek/#4-rope-rotary-position-embedding","title":"4. RoPE: Rotary Position Embedding","text":""}]}